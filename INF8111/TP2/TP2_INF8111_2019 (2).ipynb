{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/chfava/INF8111/blob/master/TP2/TP2_INF8111_2019.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "psnu-KsTFE2E"
   },
   "source": [
    "# INF8111 - Fouille de données\n",
    "\n",
    "\n",
    "## TP2 Automne 2019 - Extraction et analyse d'une base de données de tweets\n",
    "\n",
    "##### Membres de l'équipe:\n",
    "\n",
    "    - Charles-Olivier Favreau (1789479)\n",
    "    - Jean-Guillaume Langlois (1741803)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nTFRgv3VFE2F"
   },
   "source": [
    "## Présentation du problème\n",
    "\n",
    "En 2017, Twitter compte 313 millions d’utilisateurs actifs par mois avec 500 millions de tweets envoyés par jour. Cette information est rendue disponible à destination de la recherche et du développement web grâce à une API publique qui permet de collecter les informations que l'on souhaite.\n",
    "\n",
    "Néanmoins, la politique de développement de Twitter limite le partage de ces données. En effet, le partage du contenu des tweets dans une base de données n'est pas autorisé, seuls les identifiants des tweets le sont. \n",
    "Pour partager publiquement une base de données de tweets que l'on a créée, il faut que cette base de données ne soit consituée que des identifiants de tweets, et c'est ce que l'on retrouve dans la plupart des jeux de données publiques.\n",
    "\n",
    "Il est donc nécessaire pour exploiter ces données \"d'hydrater\" les tweets en question, c'est-à-dire extraire l'ensemble des informations à partir de l'ID, ce qui demande d'utiliser l'API de Twitter.\n",
    "\n",
    "Nous allons ici utiliser des bases de données publiques créées par GWU (George Washington University), qui ont l'avantage d'être très récentes : \n",
    "https://dataverse.harvard.edu/dataverse/gwu-libraries\n",
    "\n",
    "Chaque base de données de GWU couvre un sujet précis (élection américaine de 2016, jeux olympiques, etc.), et les données ont été recueillis en appliquant des requêtes qui filtraient les résultats pour n'avoir que des tweets pertinents. Un fichier README est fourni avec chaque base de données pour donner les détails de création du *dataset*. \n",
    "\n",
    "\n",
    "**Les objectifs de ce TP sont donc les suivants :**\n",
    "\n",
    " 1. Construire un *crawler* qui collecte les informations d'un tweet à partir de son ID, avec le jeu de données de son choix et les informations pertinentes pour le sujet choisi\n",
    " 2. A partir de ces données de Twitter collectés, application de méthodes en Machine Learning (ML)/Natural Language Processing (NLP) pour fournir une analyse pertinente. \n",
    "\n",
    "\n",
    "Twitter autorisant le partage **local** des données (par exemple au sein d'un groupe de recherche), une base de données sera fournie si vous ne parvenez pas à créer la vôtre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h1-ThKqWFE2G"
   },
   "source": [
    "# I/ Hydratation de tweets à l'aide de l'API Twitter (4 Pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0OBwn_P7FE2H"
   },
   "source": [
    "### 1. Obtenir l'authorisation de Twitter pour l'utilisation de l'API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dDigFeH4FE2I"
   },
   "source": [
    "Pour l'authentification, Twitter utilise OAuth : https://developer.twitter.com/en/docs/basics/authentication/overview/oauth\n",
    "Vous aurez ici besoin en particulier de OAuth2, car vous n'allez pas interagir avec des utilisateurs sur Twitter (simplement collectés des données).\n",
    "\n",
    "##### 1.1. Obtention d'un compte Twitter développeur\n",
    "\n",
    " La première étape nécessaire pour enregistrer votre application et de créer un compte Twitter développeur. Pour ce faire :\n",
    "\n",
    " - Créez un compte Twitter classique si vous n'en avez pas déjà un.\n",
    " \n",
    " - Sur le site, https://developer.twitter.com, cliquez sur *apply* pour obtenir un compte développeur. \n",
    " \n",
    " - Remplissez tous les champs nécessaires. Twitter demande beaucoup de détails sur l'utilisation que vous allez faire de ce compte, il est donc important d'expliquer la démarche en détail : il faut souligner le fait que le projet est **académique** (aucune intention commerciale, aucune publication des données collectés, etc.), expliquer les objectifs et l'apprentissage de ce TP (prise en main de l'API Twitter, l'application concrète de méthodes de Data Mining, etc.), mais aussi expliquer en détail ce que vous allez faire des données (en reprenant des consignes du sujet), les méthodes que vous allez appliquer (citez des méthodes vues en cours ou au précédent TP), le rendu fourni (insistez sur le fait que rien ne sera publique), etc. Pensez notamment à indiquer le nom du cours et le sigle du cours, le nom de l'établissement, mon nom (Théo Moins), etc. Cochez que vous n'utiliserez pas la fonctionnalité de Retweet, et que l'aggregation et l'affichage de tweets ne sera fait que dans un cadre pédagogique (non publique, et sous la forme d'un projet de recherche). Si jamais vous n'êtes pas assez précis, Twitter peut vous renvoyer un courriel pour vous demander des précisions. \n",
    "\n",
    "##### 1.2. Obtention d'un jeton d'accès\n",
    "\n",
    " - Lorsque Twitter aura validé votre demande de compte développeur, allez sur https://developer.twitter.com/en/apps pour créer une application (cliquer sur *create an app*)\n",
    "\n",
    "- Ici encore, des informations sont à fournir ici. Certaines, comme le nom ou le site internet, ne sont pas très importante, vous pouvez mettre un site internet factice si vous le souhaitez.\n",
    "\n",
    "- A la fin de ce processus, vous pouvez enfin obtenir les clés et les jetons pour utiliser l'API: allez sur la page de l'application pour créer les jetons. Vous devez récupérer une paire de clés et une paire de jetons pour passer à la suite.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZkqWyERHFE2J"
   },
   "outputs": [],
   "source": [
    "CONSUMER_KEY = \"HEcF5WyjfITgN2V7gkHVPBnSy\"\n",
    "CONSUMER_SECRET = \"gqKwr8mCSFQfLd7OBcTNktBBEJVqQ3R7ytqAayYoRV2y1YFAQq\"\n",
    "\n",
    "oauth_token = \"1186806885434966016-5ubCZog5iEiPGtSKvnxlX6et41HNdS\"\n",
    "oauth_secret = \"VkNWfhrRzcbAWmNNZu3n8D2y7ZPubuDBw0CIntmOGniGH\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FqFZWa8oFE2L"
   },
   "source": [
    "###  2. Premiers pas avec Twython\n",
    "\n",
    "##### 2.1 Installation et import de la librairie\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "acHgwXZHFE2M"
   },
   "source": [
    "Plusieurs librairies Python existent pour manipuler l'API Twitter. Aussi appelé *wrappers*, ce sont un ensemble de fonctions python qui appelle des fonctions de l'API. Parmi elles, nous utiliserons Twython, librairie répendue et activement maintenue.\n",
    "\n",
    "Documentation de Twython : https://twython.readthedocs.io/en/latest/api.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "colab_type": "code",
    "id": "DmyFmMuKFE2M",
    "outputId": "0b9e6cba-ddf5-4b5a-c4df-1bc35c61aa1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'twython'...\n",
      "remote: Enumerating objects: 3606, done.\u001b[K\n",
      "remote: Total 3606 (delta 0), reused 0 (delta 0), pack-reused 3606\n",
      "Receiving objects: 100% (3606/3606), 2.69 MiB | 0 bytes/s, done.\n",
      "Resolving deltas: 100% (1959/1959), done.\n",
      "/home/jupyter/twython\n",
      "running install\n",
      "error: can't create or remove files in install directory\n",
      "\n",
      "The following error occurred while trying to add or remove files in the\n",
      "installation directory:\n",
      "\n",
      "    [Errno 13] Permission denied: '/usr/local/lib/python2.7/dist-packages/test-easy-install-793.write-test'\n",
      "\n",
      "The installation directory you specified (via --install-dir, --prefix, or\n",
      "the distutils default setting) was:\n",
      "\n",
      "    /usr/local/lib/python2.7/dist-packages/\n",
      "\n",
      "Perhaps your account does not have write access to this directory?  If the\n",
      "installation directory is a system-owned directory, you may need to sign in\n",
      "as the administrator or \"root\" account.  If you do not have administrative\n",
      "access to this machine, you may wish to choose a different installation\n",
      "directory, preferably one that is listed in your PYTHONPATH environment\n",
      "variable.\n",
      "\n",
      "For information on other options, you may wish to consult the\n",
      "documentation at:\n",
      "\n",
      "  https://setuptools.readthedocs.io/en/latest/easy_install.html\n",
      "\n",
      "Please make the appropriate changes for your system and try again.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import time\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    from twython import Twython, TwythonError, TwythonRateLimitError\n",
    "except ImportError:\n",
    "    #!pip install --user twython\n",
    "    !git clone git://github.com/ryanmcgrath/twython.git\n",
    "    %cd twython/\n",
    "    !python setup.py install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xKYjcgsNFE2P"
   },
   "source": [
    "##### 2.2 Création d'une application et premiers tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZgiGA5WSFE2P"
   },
   "outputs": [],
   "source": [
    "from twython import Twython, TwythonError, TwythonRateLimitError\n",
    "twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET, oauth_token, oauth_secret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iOOjDlrRFE2S"
   },
   "source": [
    "Voici un test avec une recherche très simple pour vous assurer que la requête fonctionne.\n",
    "\n",
    "La fonction search renvoie une recherche (non exhaustive) de tweets, et l'option \"*popular*\" permet de retourner les résultats les plus populaires de la réponse. (documentation ici: https://developer.twitter.com/en/docs/tweets/search/api-reference/get-search-tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ddPmysQvFE2T"
   },
   "outputs": [],
   "source": [
    "basic_search = twitter.search(q='python', result_type='popular')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kzFTws0cFE2V"
   },
   "source": [
    "La fonction `search` renvoie un dictionnaire contenant la liste de tweets de la requête, et les métadonnées.\n",
    "\n",
    "Voici un exemple d'un résultat d'une recherche, observez ainsi toutes les données/métadonnées que contient un tweet et que vous pouvez extraire par la suite:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "FNUMYmwsFE2V",
    "outputId": "d8de33da-c227-466a-e017-1d08d203ca24"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'contributors': None,\n",
       " 'coordinates': None,\n",
       " 'created_at': 'Tue Nov 12 01:49:59 +0000 2019',\n",
       " 'entities': {'hashtags': [],\n",
       "  'symbols': [],\n",
       "  'urls': [{'display_url': 'twitter.com/i/web/status/1…',\n",
       "    'expanded_url': 'https://twitter.com/i/web/status/1194069797404725249',\n",
       "    'indices': [117, 140],\n",
       "    'url': 'https://t.co/aeaR0eIE9R'}],\n",
       "  'user_mentions': []},\n",
       " 'favorite_count': 70,\n",
       " 'favorited': False,\n",
       " 'geo': None,\n",
       " 'id': 1194069797404725249,\n",
       " 'id_str': '1194069797404725249',\n",
       " 'in_reply_to_screen_name': None,\n",
       " 'in_reply_to_status_id': None,\n",
       " 'in_reply_to_status_id_str': None,\n",
       " 'in_reply_to_user_id': None,\n",
       " 'in_reply_to_user_id_str': None,\n",
       " 'is_quote_status': False,\n",
       " 'lang': 'en',\n",
       " 'metadata': {'iso_language_code': 'en', 'result_type': 'popular'},\n",
       " 'place': None,\n",
       " 'possibly_sensitive': False,\n",
       " 'retweet_count': 78,\n",
       " 'retweeted': False,\n",
       " 'source': '<a href=\"https://buffer.com\" rel=\"nofollow\">Buffer</a>',\n",
       " 'text': 'Valencia, Spain has more sunshine than any other European city. No wonder I loved it here! Find out the best ways t… https://t.co/aeaR0eIE9R',\n",
       " 'truncated': True,\n",
       " 'user': {'contributors_enabled': False,\n",
       "  'created_at': 'Tue May 29 14:03:42 +0000 2012',\n",
       "  'default_profile': False,\n",
       "  'default_profile_image': False,\n",
       "  'description': 'International award-winning travel writer & photographer. Get first-hand expert travel tips and inspiration.',\n",
       "  'entities': {'description': {'urls': []},\n",
       "   'url': {'urls': [{'display_url': 'twbender.link/website-t',\n",
       "      'expanded_url': 'http://twbender.link/website-t',\n",
       "      'indices': [0, 23],\n",
       "      'url': 'https://t.co/7bMWCnfDMB'}]}},\n",
       "  'favourites_count': 121504,\n",
       "  'follow_request_sent': False,\n",
       "  'followers_count': 33742,\n",
       "  'following': False,\n",
       "  'friends_count': 32210,\n",
       "  'geo_enabled': True,\n",
       "  'has_extended_profile': False,\n",
       "  'id': 593733140,\n",
       "  'id_str': '593733140',\n",
       "  'is_translation_enabled': False,\n",
       "  'is_translator': False,\n",
       "  'lang': None,\n",
       "  'listed_count': 1110,\n",
       "  'location': 'United States',\n",
       "  'name': 'Travel With Bender',\n",
       "  'notifications': False,\n",
       "  'profile_background_color': 'C0DEED',\n",
       "  'profile_background_image_url': 'http://abs.twimg.com/images/themes/theme1/bg.png',\n",
       "  'profile_background_image_url_https': 'https://abs.twimg.com/images/themes/theme1/bg.png',\n",
       "  'profile_background_tile': False,\n",
       "  'profile_banner_url': 'https://pbs.twimg.com/profile_banners/593733140/1566899112',\n",
       "  'profile_image_url': 'http://pbs.twimg.com/profile_images/1042957256289996800/kZk8uxwA_normal.jpg',\n",
       "  'profile_image_url_https': 'https://pbs.twimg.com/profile_images/1042957256289996800/kZk8uxwA_normal.jpg',\n",
       "  'profile_link_color': '0084B4',\n",
       "  'profile_sidebar_border_color': 'FFFFFF',\n",
       "  'profile_sidebar_fill_color': 'DDEEF6',\n",
       "  'profile_text_color': '333333',\n",
       "  'profile_use_background_image': True,\n",
       "  'protected': False,\n",
       "  'screen_name': 'TravelwitBender',\n",
       "  'statuses_count': 47081,\n",
       "  'time_zone': None,\n",
       "  'translator_type': 'none',\n",
       "  'url': 'https://t.co/7bMWCnfDMB',\n",
       "  'utc_offset': None,\n",
       "  'verified': True}}"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_search['statuses'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aW0esoGGFE2X"
   },
   "source": [
    "Il est également possible avec Twython de récupérer les informations d'un tweet à partir de son ID. \n",
    "\n",
    "#### Question 1. Afficher la date, le nom d'utilisateur et le contenu du tweet ayant l'ID : 1157345692517634049 (0.5 Pts)\n",
    "\n",
    "*Indice : vous pourrez utiliser avec la fonction de twython `show_status`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vzLVdSmXFE2Y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Aug 02 17:41:30 +0000 2019\n",
      "Donald J. Trump\n",
      "A$AP Rocky released from prison and on his way home to the United States from Sweden. It was a Rocky Week, get home ASAP A$AP!\n"
     ]
    }
   ],
   "source": [
    "test_id = \"1157345692517634049\"\n",
    "\n",
    "test_tweet = twitter.show_status(id=test_id)\n",
    "\n",
    "print(test_tweet['created_at'])\n",
    "print(test_tweet['user']['name'])\n",
    "print(test_tweet['text'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6qLQNyH-FE2a"
   },
   "source": [
    "**Attention** : Twitter a une limitation de requête par fenêtre de 15 minutes, qui est donc à prendre en compte dans la base de données : https://developer.twitter.com/en/docs/basics/rate-limiting.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XpeRdeIxFE2b"
   },
   "source": [
    "### 3. Hydratation d'une base de donnée de tweets\n",
    "\n",
    "Les choses sérieuses commencent ! \n",
    "\n",
    "On souhaite désormais construire une fonction `hydrate_database` qui, à partir d'un fichier texte contenant une liste d'ID de tweets, créer un fichier csv contenant les informations que l'on souhaite extraire. \n",
    "\n",
    "Due à la limitation de requête, la fonction `show_status` vue plus haut s'avère peu efficace pour cette tâche : à raison de 900 requêtes pour 15 minutes, il sera beaucoup trop long de construire une base de données un tant soit peu conséquente. La fonction `lookup_status` (voir documentation) sera donc plus adaptée. Elle permettra d'hydrater 100 tweets par requête, ce qui, a raison d'une limite de 900 requêtes pour 15 minutes, rends la construction de la base de données plus réaliste. Il faudra tout de même gérer l'erreur générer par la limitation, si l'on souhaite avoir plus de 90000 tweets ou si l'on appelle plusieurs fois la fonction en moins de 15 minutes.\n",
    "\n",
    "#### Question 2. Implémenter la fonction `hydrate_database` (3.5 Pts)\n",
    "\n",
    "*Attention : Il faut également gérer le cas où la feature demandée n'est pas une clé du dictionnaire mais une \"sous-clé\", comme c'est le cas pour le nom d'utilisateur par exemple (accessible dans la feature *user*, qui lui même est un dictionnaire). Un moyen simple pour pallier à ce problème consiste à considérer la feature comme une liste, qui contiendrait la clé et les sous-clés si il y a lieu (voir exemple plus bas)\n",
    "\n",
    "*Indice : La fonction `sleep` du module time permet de patienter le temps nécessaire*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WHg-jnHpFE2c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def hydrate_database(filename, database_name,\n",
    "                     features, nb_requests,\n",
    "                     tweet_hydratation_limit=100):\n",
    "    \"\"\"\n",
    "    Create a csv file that contains features of tweets from an file that contains ID of tweets.\n",
    "\n",
    "    filename: Name of the file that contains ids\n",
    "    database_name: name of the file that will be created\n",
    "    features: List of features\n",
    "    nb_requests: number of time the function lookup_status will be called\n",
    "    tweet_hydratation_limit:\n",
    "    \"\"\"\n",
    "    # Opening the ID File:\n",
    "    file = open(filename, \"r\")\n",
    "    with open(database_name, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
    "\n",
    "            # TODO\n",
    "            writer = csv.writer(csvfile)\n",
    "            row = [\"Id\",\"ScreenName\",\"Text\"]\n",
    "            writer.writerow(row)\n",
    "\n",
    "            ids=[]\n",
    "            i = 0\n",
    "            requestCount = 0\n",
    "            # On itère sur les ids du fichier texte\n",
    "            for line in file:\n",
    "                single_id=line.rstrip(\"\\n\\r\")\n",
    "                ids.append(single_id)\n",
    "                i+=1\n",
    "                if (i == 100):\n",
    "                    ids_one_string=\"\"\n",
    "                    for element in ids:\n",
    "                        ids_one_string+=element\n",
    "                        ids_one_string+=','\n",
    "                    # on enlève la dernière virgule\n",
    "                    ids_one_string = ids_one_string[:-1]\n",
    "\n",
    "                    try:\n",
    "                        tweets=twitter.lookup_status(id=ids_one_string)\n",
    "                        requestCount+=1\n",
    "                        j=0\n",
    "                        for tweet in tweets:\n",
    "                            if (len(tweet)>0):\n",
    "                                # On s'assure qu'il ne s'agit pas d'un retweet\n",
    "                                if \"RT\" not in tweet['text']:\n",
    "                                    row[0]=ids[j]\n",
    "                                    row[1]=tweet['user'][\"screen_name\"]\n",
    "                                    row[2]=tweet['text']\n",
    "                                    writer.writerow(row)\n",
    "                            j+=1\n",
    "                        ids=[]\n",
    "                        i = 0\n",
    "                        print(requestCount)\n",
    "                    except TwythonError as e:\n",
    "                        if isinstance(e, TwythonRateLimitError):\n",
    "                            # retry_after = int(e.retry_after)\n",
    "                            print(\"Out of request, sleeping for 15 minutes\")\n",
    "                            time.sleep(901)\n",
    "                            print(\"finished 15min\")\n",
    "                            \n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DPR-MFGfFE2d"
   },
   "source": [
    "Utilisez le fichier suivant en guise d'example : \n",
    "https://dataverse.harvard.edu/file.xhtml?persistentId=doi:10.7910/DVN/5QCCUU/QPYP8G&version=1.1\n",
    "\n",
    "On suppose qu'on ne souhaite garder que le texte (*text*) l'ID de l'utilisateur (*user/screen_name*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JDwiRittFE2e"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'gwu/news_outlets.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-248-dddf1e2d9bda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m hydrate_database(filename, database_name, features,\n\u001b[0;32m----> 7\u001b[0;31m                  nb_requests, tweet_hydratation_limit=100)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-247-2596372f553d>\u001b[0m in \u001b[0;36mhydrate_database\u001b[0;34m(filename, database_name, features, nb_requests, tweet_hydratation_limit)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \"\"\"\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Opening the ID File:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatabase_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcsvfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'gwu/news_outlets.txt'"
     ]
    }
   ],
   "source": [
    "filename = \"gwu/news_outlets.txt\"\n",
    "database_name = \"databases/news_outlets.csv\"\n",
    "features = [['text'], ['user', 'screen_name']]\n",
    "nb_requests = 900\n",
    "\n",
    "hydrate_database(filename, database_name, features,\n",
    "                 nb_requests, tweet_hydratation_limit=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eUqa84SkFE2g"
   },
   "source": [
    "# II/ Analyse d'une base de données au choix (16 pts)\n",
    "\n",
    "Maintenant que vous êtes en mesure d'hydrater une base de données de tweets efficacement et en prenant en compte les limitations de Twitter, vous pouvez l'appliquer sur le *dataset* qui vous intéresse le plus.\n",
    "\n",
    "### 1. Instructions\n",
    "\n",
    "Dans cette partie, vous allez mener **entièrement** de vous-même un projet de *Data Science*, c'est à dire de la collecte des données jusqu'à l'interprétation des résultats. 3 sujets sont proposés, vous devez choisir celui qui vous intéresse le plus parmi :\n",
    " \n",
    " 1. Analyse de sentiments pour la prédiction des résultats de l'élection américaine. \n",
    "    \n",
    "    **Dataset :** \"2016 United States Presidential Election Tweet Ids\", https://doi.org/10.7910/DVN/PDI7IN  \n",
    "    \n",
    "    **Précision :** Ce sujet est assez similaire au TP1 (avec ici sentiment = parti politique), vous êtes donc libre de reprendre ce que vous aviez fait. Cependant, il faudrait aller un peu plus en profondeur ici, par exemple sur l'étape de la classification. De plus, vous avez ici une nouvelle problématique qui est que vos données ne sont pas labellisés (mais la construction des collections devrait vous permettre de labelliser vous-même).\n",
    " \n",
    " \n",
    " 2. Détection de discours d'incitation à la haine.\n",
    "    \n",
    "    **Dataset :** Modifier votre fonction d'hydratation en utilisant la fonction search pour n'avoir que des tweets récents.\n",
    " \n",
    "     **Précision :** Ce sujet pourrait également être abordé de la même manière que le TP1 : des étapes de preprocessing + de la classification. Néanmoins, dans ce cas, posséder des données avec des labels \"incitant à la haine\"/\"n'insite pas à la haine\" est beaucoup plus complexe, car beaucoup de bases de données étiquetés, lors de l'hydratation, se trouveront être quasi-vide, car les tweets auront été supprimés au moment où nous ferons notre requête (car Twitter veille aussi à la suppression de tweets haineux). C'est pourquoi vous êtes obligés de créer une base de données avec des tweets les plus récents possibles, avant qu'ils ne soient potentiellement supprimés. Pour désigner un tweet comme haineux, une méthode serait la détection de vocabulaire haineux, par exemple avec `hatebase.org`, qui propose des larges bases de données très complètes. Vous pouvez créer un compte sur le site pour avoir accès à l'API, et ensuite utiliser cette librairie pour Python : https://github.com/DanielJDufour/hatebase. En modifiant la requête pour n'avoir que des tweets contenant ce vocabulaire, et en le mêlant à de l'analyse de sentiment, vous pourrez obtenir des résultats à analyser. Vous pourriez aussi avoir une approche \"utilisateur\" pour rechercher des tweets haineux : lorsqu'un tweet est détecter comme haineux, inspecter l'ensemble des tweets de l'utilisateur et/ou de ses *followers*. En bref, beaucoup de possibilités, mais ce sujet est le plus complexe des trois. Je serai donc moins exigeant sur les résultats 'chiffrés', l'important ici étant plus l'analyse, et le fait d'avoir une approche cohérente (il est également très important de prendre le temps de réfléchir à une définition claire de \"haineux\").\n",
    "\n",
    "\n",
    " 3. Méthodes de clusterings appliqué au tweet sur l'actualité, et analyse des résultats. \n",
    "    \n",
    "    **Dataset :** \"News Outlet Tweet Ids\", https://doi.org/10.7910/DVN/2FIFLH\n",
    "\n",
    "    **Précision :** Application de méthodes de preprocessing, puis de méthodes de clustering pour regrouper les tweets qui mentionnent la même actualité ou catégorie d'actualité (au choix!), puis visualisation, étude en fonction du temps...  Vous devrez trouver quelle est la meilleur méthode de clustering, et celle-ci dépendra de votre approche (nombre de classes connu ? si oui, combien de classes?). \n",
    "    \n",
    "    \n",
    "Vous êtes entièrement libre sur l'ensemble du processus (choix des informations extraites, méthodes en ML, librairie, etc.). Ici seul les bases de données en elle-même sont rigoureusement imposés. Les précisions faites ici servent juste pour vous guider un peu si vous le souhaitez, mais si vous avez d'autres idées n'hésitez pas ! Ces sujets étant populaires au sein de la communauté scientifique, vous pouvez (**seulement si vous le souhaitez**) vous inspirer d'articles de la littérature, à condition de le citer dans votre rapport et de faire votre propre implémentation. \n",
    "\n",
    "#### L'objectif cependant ici n'est pas d'obtenir l'état de l'art, mais d'appliquer une méthodologie claire et rigoureuse que vous aurez construite vous-même. \n",
    "\n",
    "Les datasets étant massifs, il est fortement déconseillé de faire une base de données contenant tous les tweets hydratés (par exemple, les auteurs de la BDD n°1 soulignent qu'avec les limitations de l'API cela vous prendrait environ 32 jours). C'est à vous de voir quelle est la taille du dataset dont vous avez besoin.\n",
    "\n",
    "Pensez aussi à lire le fichier README correspondant à la base que vous avez choisi, afin de vous aider à mieux comprendre vos futurs résultats.\n",
    "\n",
    "### 2. Rédaction d'un rapport\n",
    "\n",
    "Pour ce TP, vous allez devoir fournir un rapport qui détail et justifie l'ensemble de votre méthode, et qui fournisse les résultats que vous avez obtenus. Les éléments suivants doivent y apparaitre (cela peut vous servir de plan, mais ce n'est pas rigide) :\n",
    "\n",
    "- Titre du projet, et nom de l'ensemble des membres de l'équipe (avec mail et matricule)\n",
    "    \n",
    "- **Introduction** : résumé du problème, de la méthodologie et des résultats obtenus.\n",
    "\n",
    "- **Présentation du dataset** : description, justification de la taille, du choix des features, etc. \n",
    "\n",
    "- **Preprocessing** : s'il y en a, justification des étapes de preprocessing.\n",
    "\n",
    "- **Methodologie** : description et justification de l'ensemble des choix (algorithmes, hyper-paramètres, régularisation, métriques, etc.)\n",
    "\n",
    "- **Résultats** : analyse des résultats obtenus (utilisez des figures pour illustrer), mise en relation entre les choix de design et la performance obtenue.\n",
    "\n",
    "- **Discussion** : discutez des avantages et des inconvénients de votre approche; quels sont les faiblesses, les failles ? Qu'est-ce qu'il peut être amélioré ? Vous pouvez également suggérer des futures idées d'exploration.\n",
    "\n",
    "- **Références** : si vous vous êtes inspiré d'une étude déjà faite.\n",
    "    \n",
    "Vous pouvez utiliser le template d'arXiv pour le rapport : https://fr.overleaf.com/latex/templates/style-and-template-for-preprints-arxiv-bio-arxiv/fxsnsrzpnvwc. **L'ensemble du rapport ne doit cependant pas excéder 5 pages, figures et références compris.** Les 5 pages ne sont pas obligatoires, si vous estimez que moins est suffisant et que votre rapport est effectivement complet, vous ne serez pas pénalisé.\n",
    "\n",
    "\n",
    "### 3. Rendu attendu\n",
    "\n",
    "A la fin du TP, vous soumettrez un fichier *zip* contenant les éléments suivants:\n",
    "\n",
    "- Le fichier *pdf* du rapport\n",
    "- Ce notebook que vous aurez complété. Vous pouvez également implémenter votre méthode à la suite ici, ou alors utiliser un autre fichier si vous le souhaitez. Bien que seul le rapport servira pour la notation, ayez un code commenté et clair !\n",
    "- Ne pas envoyer les fichiers de données, car trop conséquent. Avec le rapport et le code, tout sera détaillé et il sera possible de les refaire facilement.\n",
    "\n",
    "### 4. Evalutation\n",
    "\n",
    "12 points de cette partie sera basé sur la méthodologie, et 4 points sur les résultats.\n",
    "\n",
    "La notation sur la méthodologie inclus : \n",
    "\n",
    "- La pertinence de l'ensemble des étapes de l'approche\n",
    "\n",
    "- La bonne description des algorithmes choisis\n",
    "\n",
    "- La justification judicieuse des choix établis\n",
    "\n",
    "- Une analyse pertinente des résultats\n",
    "\n",
    "- La clarté et l'organisation du rapport (figures, tables) et du code.\n",
    "\n",
    "\n",
    "Pour ce qui est des résultats, il est impossible de mettre un barème fixe car ils vont dépendre du sujet que vous allez choisir. C'est un problème auquel vous serez confrontés : chaque étude étant spécifique, il peut être compliqué d'évaluer qualitativement un modèle, d'autant que vous n'avez sans doute pas connaissance de l'état de l'art. C'est pourquoi il va être important de faire plusieurs essais, et de comparer différentes méthodes. Ainsi, les résultats doivent être cohérent avec la complexité de votre implémentation : un modèle simple et naïf vous fournira des premiers résultats, que vous devrez ensuite améliorer avec des modèles plus précis et complexes.\n",
    "\n",
    "De ce fait, l'ensemble des points pour les résultats seront donnés si : \n",
    " - Vous obtenez des premiers résultats avec une méthode naïve qui témoignent de la pertinence de vos choix \n",
    " - Ces résultats sont ensuite améliorés avec une méthode plus complexe\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ztyy5MqPc4P3"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5gI3RApAFE2h"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zHhkNkV5c2J8"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"news_outlets (3).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "wH4LU-r1kkKL",
    "outputId": "85a17e6f-9292-4643-cc66-cc729d8268aa"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwordsEnglish = stopwords.words('english')\n",
    "stopwordsEnglish.append(\"via\")\n",
    "stopwordsEnglish.append(\"say\")\n",
    "stopwordsEnglish.append(\"says\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "2c5YA76MlVfH",
    "outputId": "d0d50368-1430-473b-ac71-3c10b6e85c88"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "\n",
    "tweetToeknizer = TweetTokenizer()\n",
    "\n",
    "def custom_preprocessor(input):\n",
    "    input = input.lower()\n",
    "    input = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', input, flags=re.MULTILINE)\n",
    "    return input\n",
    "\n",
    "def custom_tokenizer(input):\n",
    "    tokens = tweetToeknizer.tokenize(input)\n",
    "    tokens = [word for word in tokens if not word in stopwordsEnglish]\n",
    "    tokens = [word for word in tokens if not word in stopwords.words('french')]\n",
    "    tokens = [word for word in tokens if not word in stopwords.words('spanish')]\n",
    "    tokens=[word for word in tokens if word.isalpha()]\n",
    "    tokens = [word for word in tokens if not word in string.punctuation]\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oLsC4gLDdk7f"
   },
   "outputs": [],
   "source": [
    "TF_IDF = TfidfVectorizer(preprocessor=custom_preprocessor, tokenizer=custom_tokenizer)\n",
    "\n",
    "X = TF_IDF.fit_transform(df['Text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lJgaBe0oPvoY"
   },
   "outputs": [],
   "source": [
    "def getMostFrequentWords(X, df, clusterIndex, n):\n",
    "    indexes = np.where(df['Class'].values == clusterIndex)\n",
    "    dataForClass = X[indexes]\n",
    "    \n",
    "    TF_IDF_for_class = TfidfVectorizer(preprocessor=None, tokenizer=None)\n",
    "    TF_IDF_for_class.fit(dataForClass)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMostFrequentWordsPerClass(X, y, features, n):\n",
    "    df_list = []\n",
    "    labels = np.unique(y)\n",
    "    for label in labels:\n",
    "        indexes = np.where(y==label)\n",
    "        \n",
    "        SubData = X[indexes].toarray()\n",
    "        SubData[SubData < 0.1] = 0\n",
    "        tfidfMeans = np.mean(SubData, axis=0)\n",
    "        \n",
    "        \n",
    "        n_most_ids = np.argsort(tfidfMeans)[::-1][:n]\n",
    "        top = [(features[i], tfidfMeans[i]) for i in n_most_ids]\n",
    "        df = pd.DataFrame(top)\n",
    "        df.columns = ['feature', 'tfidf']\n",
    "        df.label = label\n",
    "        df_list.append(df)\n",
    "    return df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Kr7z4YPPk0wU",
    "outputId": "63b73448-c83a-4908-d512-753f56beeec1"
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nGm1tHFuorNn"
   },
   "source": [
    "## Clustering avec KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "weU_goVCoZxY"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "COEBjuchoylz"
   },
   "outputs": [],
   "source": [
    "kmeans = MiniBatchKMeans(n_clusters=5,batch_size=100).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualiseClusters(X, kmeans.labels_, kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t3X_Rhe4tlFK"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "def visualiseClusters(X, clusters, centroids = None):\n",
    "  #data = PCA(n_components=2).fit_transform(X)\n",
    "  SVD = TruncatedSVD(n_components=2)\n",
    "  data = SVD.fit_transform(X)\n",
    "  if centroids is not None:\n",
    "    centroids = SVD.transform(centroids)\n",
    "  plt.scatter(data[:,0], data[:,1], c = clusters)\n",
    "  if centroids is not None:\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1], marker='*', c='r', label='centroids')\n",
    "  plt.legend()\n",
    "  plt.show()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3qQy0doP5UXz"
   },
   "source": [
    "## Utiliser la méthode du coude pour trouver le bon numbre de clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "colab_type": "code",
    "id": "984p-VJTzcXx",
    "outputId": "43881f47-2f2b-4c3b-ebf7-c5788028eeba"
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "max_cluster_number = 15\n",
    "for cluster_count in range(1,max_cluster_number):\n",
    "    kmeans = MiniBatchKMeans(n_clusters=cluster_count, batch_size=100).fit(X)\n",
    "    results.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(range(1, max_cluster_number), results, marker='o')\n",
    "plt.xlabel('cluster Count')\n",
    "plt.ylabel('in-cluster sum of squares')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 609
    },
    "colab_type": "code",
    "id": "X8ehTM_G1qQ5",
    "outputId": "a0974c9b-c573-42f3-9dc6-5937c19bcf53"
   },
   "outputs": [],
   "source": [
    "for cluster_count in range(1,10):\n",
    "  kmeans = MiniBatchKMeans(n_clusters=cluster_count, batch_size=100).fit(X)\n",
    "  visualiseClusters(X, kmeans.labels_, kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mostFrequentWords = getMostFrequentWordsPerClass(X, kmeans.labels_, TF_IDF.get_feature_names(), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, cluster_words in enumerate(mostFrequentWords):\n",
    "    print(\"Cluster #\" + str(index))\n",
    "    print(cluster_words)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VpfWvvM47_kL"
   },
   "source": [
    "# DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bwDLhdLf5TUT"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AJ5-GMev8LhB"
   },
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=1.000000000000001, min_samples=1, n_jobs = -1).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q0jtMYnW8X5_"
   },
   "outputs": [],
   "source": [
    "dbscan.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ByTJJ4eaEI7A"
   },
   "outputs": [],
   "source": [
    "len(set(dbscan.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vXBKTJKX8aK_"
   },
   "outputs": [],
   "source": [
    "visualiseClusters(X, dbscan.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isDone = False\n",
    "eps=1.01\n",
    "delta = 0.01\n",
    "loopCounter1 = 0 \n",
    "loopCounter2 = 0 \n",
    "while not isDone:\n",
    "    print(eps)\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=1, n_jobs = -1).fit(X)\n",
    "    nbCluster = len(set(dbscan.labels_))\n",
    "    print(nbCluster)\n",
    "    if loopCounter1 == 2 and loopCounter2 == 2:\n",
    "        delta = delta / 2\n",
    "        loopCounter1 = 0\n",
    "        loopCounter2 = 0\n",
    "    if nbCluster < 2:\n",
    "        eps = eps - delta\n",
    "        if loopCounter1 == 1 and loopCounter2 == 1:\n",
    "            loopCounter1 += 1\n",
    "        else:\n",
    "            loopCounter1 += 1\n",
    "            loopCounter2 = 0\n",
    "    elif nbCluster > 15:\n",
    "        eps = eps + delta\n",
    "        if loopCounter1 == 1 and loopCounter2 == 1:\n",
    "            loopCounter2 += 1\n",
    "        else:\n",
    "            loopCounter2 += 1\n",
    "            loopCounter1 = 0\n",
    "    else :\n",
    "        isDone = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7VwamonF89wT"
   },
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showTopWordsPerCluster(model, voc, nb_words):\n",
    "    for index, topic in enumerate(model.components_):\n",
    "        print (\"Topic \" + str(index))\n",
    "        print(\" \".join([voc[i] for i in topic.argsort()[:-nb_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components = 7, max_iter=5, \n",
    "                                learning_method='online', learning_offset=50.,random_state=0).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showTopWordsPerCluster(lda, TF_IDF.get_feature_names(), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nb_cluster in range(7,15):\n",
    "    print(\"Number of Clusters :\" + str(nb_cluster))\n",
    "    lda = LatentDirichletAllocation(n_components = nb_cluster, max_iter=5, \n",
    "                                    learning_method='online', learning_offset=50.,random_state=0).fit(X)\n",
    "    showTopWordsPerCluster(lda, TF_IDF.get_feature_names(), 5)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Copy of TP2_INF8111_2019.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
