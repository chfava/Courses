{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "Copy of TP1_INF8111_2019.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chfava/INF8111/blob/master/TP1/TP1_INF8111_2019.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zps4WX5Ga_7D",
        "colab_type": "text"
      },
      "source": [
        "# INF8111 - Fouille de données\n",
        "\n",
        "\n",
        "## TP1 Automne 2019 - Preprocessing de tweets pour de l'analyse de sentiments\n",
        "\n",
        "##### Membres de l'équipe:\n",
        "\n",
        "    - Charles-Olivier Favreau (1789479)\n",
        "    - Jean-Guillaume Langlois (1741803)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BG2ZB93Ra_7F",
        "colab_type": "text"
      },
      "source": [
        "## Présentation\n",
        "\n",
        "Twitter est un réseau social permettant aux utilisateurs de publier des informations et communiquer entre eux par le biais de messages, appelés tweets, pouvant contenir jusqu'à 280 caractères. Largement utilisé aujourd'hui, ce réseau peut être un outil pour des entreprises qui souhaitent évaluer l'avis de leurs clients.\n",
        "\n",
        "Dans ce TP, on se met à la place d'une compagnie aérienne, qui souhaiterait détecter les tweets qui la mentionnent et analyser si ce sont des mentions positives ou négatives, en comparant leur résultat avec les autres compagnies.\n",
        "\n",
        "Le *preprocessing* est une tâche cruciale en fouille de données. Elle permet de transformer les données brutes en un format adapté à l'application de méthodes de machine learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hide_input": true,
        "id": "18fJ6vVBa_7G",
        "colab_type": "text"
      },
      "source": [
        "# I/ Analyse de sentiments (13 Pts)\n",
        "\n",
        "Usuellement dans la littérature, la tâche d'extraire le sentiment d'un texte est appelé *sentiment analysis*.\n",
        "Ici pour se faire, nous allons utiliser un modèle *bag-of-words* (BoW).\n",
        "\n",
        "## 1. Installation\n",
        "\n",
        "Pour ce TP, vous aurez besoin des librairies `numpy`, `sklearn` et `scipy` (que vous avez sans doute déjà), ainsi que la librairie `nltk`, qui est une libraire utilisée pour faire du traitement du language (Natural Language Processing, NLP)\n",
        "\n",
        "Installez les libraires en question et exécutez le code ci-dessous :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wbxWY3Sa_7G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "4ed45c4d-70ae-49f6-bf9f-c17161bd5759"
      },
      "source": [
        "# If you want, you can use anaconda and install after nltk library\n",
        "# !pip install --user numpy\n",
        "# !pip install --user sklearn\n",
        "# !pip install --user scipy\n",
        "# !pip install --user nltk\n",
        "\n",
        "import sys\n",
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"averaged_perceptron_tagger\")\n",
        "nltk.download(\"universal_tagset\")\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_o38pY9a_7J",
        "colab_type": "text"
      },
      "source": [
        "## 2. Jeu de données\n",
        "\n",
        "On utilise un jeu de donnée provenant de *Crowdflower's Data for Everyone library*: https://www.figure-eight.com/data-for-everyone/\n",
        "\n",
        "Pour citer la source originale de la base :\n",
        "\n",
        "    A sentiment analysis job about the problems of each major U.S. airline. Twitter data was scraped from February of 2015 and contributors were asked to first classify positive, negative, and neutral tweets, followed by categorizing negative reasons (such as \"late flight\" or \"rude service\").\n",
        "\n",
        "Les compagnies incluses dans cette base de données sont Virgin America, United Airline, Southwest Airlines, jetBlue, USAirways, et American Airlines.\n",
        "\n",
        "Dans le fichier zip du TP, vous trouverez le fichier *airline_tweets_database.csv*, qui est la base de données de tweets que nous allons manipuler.\n",
        "\n",
        "Chaque ligne de ce fichier contient un tweet, avec plusieurs informations : l'identifiant du tweet, l'utilisateur, le contenu, le nombre de retweet... Ainsi que le label.\n",
        "\n",
        "3 labels différents sont possibles dans ce dataset : *négatif*, *neutre* et *positif*, que l'on va représenter respectivement par 0, 1 et 2.\n",
        "\n",
        "Pour ce TP, on ne va conserver ici que le texte et le label. On divise ensuite la base de données en 3 ensembles (entrainement/validation/test). Vous utiliserez l'ensemble d'entraînement et de validation pour cette partie, et l'ensemble de test à la partie suivante.\n",
        "\n",
        "Le code ci-dessous permet de charger ces ensembles:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HABdGTcFa_7K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "54ead0b2-3b3e-45eb-c425-022bb71ef3e7"
      },
      "source": [
        "import csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def load_dataset(path):\n",
        "    \n",
        "    x = []\n",
        "    y = []\n",
        "\n",
        "    with open(path, 'r', newline='', encoding=\"latin-1\") as csvfile:\n",
        "        \n",
        "        reader = csv.reader(csvfile, delimiter=',')\n",
        "        \n",
        "        # Taking the header of the file + the index of useful columns:\n",
        "        header = next(reader)\n",
        "        ind_label = header.index('airline_sentiment')\n",
        "        ind_text = header.index('text')\n",
        "        \n",
        "        for row in reader:\n",
        "            x.append(row[ind_text])\n",
        "            \n",
        "            label = row[ind_label]\n",
        "            \n",
        "            if label == \"negative\":\n",
        "                y.append(0)\n",
        "            elif label == \"neutral\":\n",
        "                y.append(1)\n",
        "            elif label == \"positive\":\n",
        "                y.append(2)\n",
        "\n",
        "        assert len(x) == len(y)\n",
        "\n",
        "        return x, y\n",
        "\n",
        "\n",
        "# Path of the dataset\n",
        "path = \"airline_tweets_database.csv\"\n",
        "\n",
        "X, y = load_dataset(path)\n",
        "\n",
        "train_valid_X, test_X, train_valid_Y, test_Y = train_test_split(X, y, test_size=0.15, random_state=12)\n",
        "\n",
        "train_X, valid_X, train_Y, valid_Y = train_test_split(train_valid_X, train_valid_Y, test_size=0.18, random_state=12)\n",
        "\n",
        "print(\"Length of training set : \", len(train_X))\n",
        "print(\"Length of validation set : \", len(valid_X))\n",
        "print(\"Length of test set : \", len(test_X))\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of training set :  10204\n",
            "Length of validation set :  2240\n",
            "Length of test set :  2196\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJ7SobuCa_7M",
        "colab_type": "text"
      },
      "source": [
        "## 3. Preprocessing\n",
        "\n",
        "Nous allons ici implémenter la *tokenization* et le *stemming*, qui sont 2 étapes courantes de preprocessing en NLP. Ensuite, afin d'avoir un modèle qui s'adapte mieux au format de Twitter, nous ajouterons une étape spécifique supplémentaire.\n",
        "\n",
        "### 3.1. Tokenization\n",
        "\n",
        "Cette étape permet de séparer un texte en séquence de *tokens* (= jetons, ici des mots, symboles ou ponctuation).\n",
        "\n",
        "Par exemple la phrase *\"It's the student's notebook.\"* peut être séparé en liste de tokens de cette manière: [\"it\", \" 's\", \"the\", \"student\", \" 's\", \"notebook\", \".\"].\n",
        "\n",
        "**De plus, tous les tokenizers ont également le rôle de mettre le texte en minuscule.**\n",
        "\n",
        "\n",
        "##### Question 1. Implémentez les 2 tokenizers différents suivants: (0.5 Pts)\n",
        "\n",
        "- Le **SpaceTokenizer**, qui est un tokenizer naïf qui sépare simplement en fonction des espaces.\n",
        "- Le **NLTKTokenizer**, qui utilise la méthode du package *nltk* (https://www.nltk.org/api/nltk.html).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xymA0vfIa_7M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "class SpaceTokenizer(object):\n",
        "    \"\"\"\n",
        "    It tokenizes the tokens that are separated by whitespace (space, tab, newline). \n",
        "    We consider that any tokenization was applied in the text when we use this tokenizer.\n",
        "\n",
        "    For example: \"hello\\tworld of\\nNLP\" is split in ['hello', 'world', 'of', 'NLP']\n",
        "    \"\"\"\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        # TODO\n",
        "\n",
        "        # Have to return a list of tokens\n",
        "        tokens = text.split()\n",
        "        return tokens\n",
        "\n",
        "\n",
        "class NLTKTokenizer(object):\n",
        "    \"\"\"\n",
        "    This tokenizer uses the default function of nltk package (https://www.nltk.org/api/nltk.html) to tokenize the text.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.tokenizer = TweetTokenizer()\n",
        "        \n",
        "    def tokenize(self, text):\n",
        "        # TODO\n",
        "\n",
        "        # Have to return a list of tokens\n",
        "        \n",
        "        tokens = self.tokenizer.tokenize(text)\n",
        "       \n",
        "        return tokens\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iKsbs84a_7O",
        "colab_type": "text"
      },
      "source": [
        "#### Testez les deux tokenizers. Quelles différences pouvez-vous constater?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7L-TIsjqa_7P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "d2c2d2a7-edd1-4814-e7e7-6b4662775636"
      },
      "source": [
        "spaceTokenizer = SpaceTokenizer()\n",
        "nltkTokenizer = NLTKTokenizer()\n",
        "\n",
        "print(\"SpaceTokenizer : \")\n",
        "print(spaceTokenizer.tokenize(train_X[0]))\n",
        "print(\"\\n\" + \"NLTKTokenizer : \")\n",
        "print(nltkTokenizer.tokenize(train_X[0]))\n",
        "\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SpaceTokenizer : \n",
            "['@USAirways', 'tells', 'me', 'to', 'talk', 'to', '@AmericanAir', 'about', 'my', 'delayed', 'flights.', 'AA', 'tells', 'me', 'to', 'talk', 'to', 'US.', '#ihatemergers']\n",
            "\n",
            "NLTKTokenizer : \n",
            "['@USAirways', 'tells', 'me', 'to', 'talk', 'to', '@AmericanAir', 'about', 'my', 'delayed', 'flights', '.', 'AA', 'tells', 'me', 'to', 'talk', 'to', 'US', '.', '#ihatemergers']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZNYQ7NdhstS",
        "colab_type": "text"
      },
      "source": [
        "Comme il est possible d'observer, les deux tokenizer ne donnent pas exactement la même sortie. Le tokenizer de NLTK isole les signes de ponctuation (.) comme un token en soi. À l'inverse, le SpaceTokenizer ne sépare pas les signes de ponctuation des mots. Ainsi, plusieurs tokens contiennent des signes de ponctuation à la fin. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuaKD9pga_7R",
        "colab_type": "text"
      },
      "source": [
        "### 3.2. Troncature (ou Stemming)\n",
        "\n",
        "Dans les phrases \"I should have bought a new shoes today\" et \"I spent too much money buying games\", les mots \"buying\" et \"bought\" représentent la même idée. Considérer ces deux mots comme différents ne ferait qu'augmenter pour rien la complexité et la dimension du problème, ce qui peut avoir un impact négatif sur la performance globale. Ainsi, on peut donc plutôt une forme unique (comme la racine du mot) pour représenter ces deux mots de la même manière. Ce procédé de conversion de mots en racines permettant de réduire la dimension est appelé usuellement *stemming*, que l'on peut traduire par troncature.\n",
        "\n",
        "\n",
        "#### Question 2. Récupérez les troncatures des tokens en utilisant l'attribut *stemmer* de la classe *Stemmer* (0.5 Pts) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h17XD1gaa_7S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "\n",
        "class Stemmer(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
        "\n",
        "    def stem(self, token):\n",
        "        \"\"\"\n",
        "        token: a string that contain a token\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "        \n",
        "        # Have to return the stemmed token\n",
        "        \n",
        "        return self.stemmer.stem(token)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDIMA2GNa_7U",
        "colab_type": "text"
      },
      "source": [
        "### 3.3. Twitter preprocessing\n",
        "\n",
        "Parfois, appliquer uniquement ces deux étapes ne suffit pas, due aux particularités des données que nous manipulons, qui peuvent demander des étapes de preprocessing spécifiques afin d'obtenir un modèle plus adapté.\n",
        "\n",
        "Couramment en NLP, un dictionnaire est utilisé pour stocker un ensemble de mots, et tous les mots n'appartenant pas au dictionnaire sont considérés comme inconnus. Ainsi, avec ce choix d'implémentation, la dimension de l'espace caractéristique du modèle est directement liée au nombre de mots du dictionnaire. Ainsi, pour des raisons de complexité mais aussi car les modèles à trop grande dimension peuvent souffrir du fléau de la dimensionnalité, il est préférable de réduire la taille de notre vocabulaire.\n",
        "\n",
        "#### Question 3. Donnez, en expliquant brièvement, au moins deux exemples d'étapes de préprocessing qui permettent de réduire la taille du dictionnaire ici, puis implémentez-les.  (2.0 points)\n",
        "\n",
        "Ces étapes de préprocessing doivent être en rapport aux charactéristiques spécifiques des données de Twitter. La suppression des mots vides ne compte pas comme une des deux étapes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_u0MrpBa_7V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "\n",
        "class TwitterPreprocessing(object):\n",
        "  \n",
        "    \n",
        "\n",
        "    def preprocess(self, tweet):\n",
        "        \"\"\"\n",
        "        tweet: original tweet\n",
        "        \"\"\"\n",
        "        # TODO : Write your preprocessing steps here.\n",
        "\n",
        "        # return the preprocessed twitter\n",
        "        \n",
        "        #minuscule\n",
        "        tokens = [token.lower() for token in tweet]\n",
        "        \n",
        "        #Normalization\n",
        "        #example : b4 -> before\n",
        "        #otw -> on the way\n",
        "        #gooood -> good\n",
        "        \n",
        "        #A TERMINER\n",
        "        \n",
        "        #remplacer dates par <DATE>\n",
        "        \n",
        "        new_tweet = tokens\n",
        "        \n",
        "        return new_tweet\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nc-LpVgSa_7X",
        "colab_type": "text"
      },
      "source": [
        "### 3.4.  Pipeline\n",
        "\n",
        "Une pipeline permet d'exécuter séquentiellement toutes les étapes de preprocessing, pour transformer les données brutes en une version utilisable pour notre modèle. La *PreprocessingPipeline* a été implémenter pour appliquer à la suite le tokenizer, les troncatures et le preprocessing spécifique à Twitter. \n",
        "\n",
        "**N'hésitez pas à changer l'ordre des étapes de preprocessing si vous le souhaitez.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahByACoFa_7Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PreprocessingPipeline:\n",
        "\n",
        "    def __init__(self, tokenization, twitterPreprocessing, stemming):\n",
        "        \"\"\"\n",
        "        tokenization: enable or disable tokenization.\n",
        "        twitterPreprocessing: enable or disable twitter preprocessing.\n",
        "        stemming: enable or disable stemming.\n",
        "        \"\"\"\n",
        "\n",
        "        self.tokenizer = NLTKTokenizer() if tokenization else SpaceTokenizer()\n",
        "        self.twitterPreprocesser = TwitterPreprocessing(\n",
        "        ) if twitterPreprocessing else None\n",
        "        self.stemmer = Stemmer() if stemming else None\n",
        "\n",
        "    def preprocess(self, tweet):\n",
        "        \"\"\"\n",
        "        Transform the raw data\n",
        "\n",
        "        tokenization: boolean value.\n",
        "        twitterPreprocessing: boolean value. Apply the\n",
        "        stemming: boolean value.\n",
        "        \"\"\"\n",
        "\n",
        "        tokens = self.tokenizer.tokenize(tweet)\n",
        "\n",
        "        if self.stemmer:\n",
        "            tokens = list(map(self.stemmer.stem, tokens))\n",
        "\n",
        "        if self.twitterPreprocesser:\n",
        "            tokens = self.twitterPreprocesser.preprocess(tokens)\n",
        "\n",
        "        return tokens\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-bv6OtZa_7b",
        "colab_type": "text"
      },
      "source": [
        "Test de la pipeline :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rii-rffIa_7c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "714a5498-2bc1-403e-ddf8-c56e08f39195"
      },
      "source": [
        "pipeline = PreprocessingPipeline(tokenization=True, twitterPreprocessing=True, stemming=True)\n",
        "print(list(map(pipeline.preprocess, train_X[:1])))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['@usairway', 'tell', 'me', 'to', 'talk', 'to', '@americanair', 'about', 'my', 'delay', 'flight', '.', 'aa', 'tell', 'me', 'to', 'talk', 'to', 'us', '.', '#ihatemerg']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEO4W187a_7i",
        "colab_type": "text"
      },
      "source": [
        "## 4. N-grams\n",
        "\n",
        "Un n-gram est une séquence continue de *n* tokens dans un texte. Par exemple, les séquences *\"nous a\"* et *\"la porte\"* sont deux exemples de 2-grams de la phrase *\"Il nous a dit au revoir en franchissant la porte.\"*. 1-gram, 2-gram et 3-gram sont respectivement appelés unigram, bigram et trigram. \n",
        "\n",
        "Voici la liste de tous les unigrams, bigrams et trigrams possible pour la phrase *\"Il nous a dit au revoir en franchissant la porte.\"* :\n",
        "- Unigram: ['Il', 'nous', 'a', 'dit', 'au', 'revoir', 'en', 'franchissant', 'la', 'porte']\n",
        "- Bigram: ['Il nous', 'nous a', 'a dit', 'dit au', 'au revoir', 'revoir en', 'en franchissant', 'franchissant la', 'la porte']\n",
        "- Trigram: ['Il nous a', 'nous a dit', 'a dit au', 'dit au revoir', 'au revoir en', 'revoir en franchissant', 'en franchissant la', 'franchissant la porte']\n",
        "\n",
        "\n",
        "##### Question 4. Implementez les fonctions `bigram` et `trigram`. (1 Pt)\n",
        "\n",
        "Vous devez résoudre cette question sans utiliser de libraire exterieur comme scikit-learn par exemple."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNpSFWDLa_7j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bigram(tokens):\n",
        "    \"\"\"\n",
        "    tokens: a list of strings\n",
        "    \"\"\"\n",
        "    # TODO\n",
        "    \n",
        "    # This function returns the list of bigrams\n",
        "    bigrams = []\n",
        "    for tokenIndex in range (0, len(tokens) - 1):\n",
        "      bigram = tokens[tokenIndex] + \" \" + tokens[tokenIndex + 1]\n",
        "      bigrams.append(bigram)\n",
        "    return bigrams\n",
        "\n",
        "\n",
        "def trigram(tokens):\n",
        "    \"\"\"\n",
        "    tokens: a list of strings\n",
        "    \"\"\"\n",
        "    # TODO\n",
        "    \n",
        "    trigrams = []\n",
        "    for tokenIndex in range (0, len(tokens) - 2):\n",
        "      trigram = tokens[tokenIndex]  + \" \" + tokens[tokenIndex + 1] + \" \" + tokens[tokenIndex + 2]\n",
        "      trigrams.append(trigram)\n",
        "    \n",
        "    # This function returns the list of trigrams\n",
        "    return trigrams\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFoUg-cQa_7l",
        "colab_type": "text"
      },
      "source": [
        "## 5. Bag-of-words\n",
        "\n",
        "Régressions logistiques, SVM et d'autres modèles très courants demande des entrées qui soient toutes de la même taille, ce qui n'est forcément le cas pour des types de données comme les textes, qui peuvent avoir un nombre variable de mots.  \n",
        "\n",
        "Par exemple, considérons la phrase 1, ”Board games are much better than video games” et la phrase 2, ”Pandemic is an awesome game!”. La table ci-dessous montre un exemple d'un moyen de représentation de ces deux phrases en utilisant une représentation fixe : "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROGeshlBa_7l",
        "colab_type": "text"
      },
      "source": [
        "|<i></i>     | an | are | ! | pandemic | awesome | better | games | than | video | much | board | is | game |\n",
        "|------------|----|-----|---|----------|---------|--------|-------|------|-------|------|-------|----|------|\n",
        "| Sentence 1 | 0  | 1   | 0 | 0        | 0       | 1      | 2     | 1    | 1     | 1    | 1     | 0  | 0    |\n",
        "| Sentence 2 | 1  | 0   | 0 | 1        | 1       | 0      | 0     | 0    | 0     | 0    | 0     | 1  | 1    |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUU7_-s8a_7m",
        "colab_type": "text"
      },
      "source": [
        "Chaque colonne représente un mot du vocabulaire (de longueur 13), tandis que chaque ligne contient l'occurence des mots dans une phrase. Ainsi, la valeur 2 à la position (1,7) est due au fait que le mot *\"games\"* apparait deux fois dans la phrase 1. \n",
        "\n",
        "Ainsi, chaque ligne étant de longueur 13, on peut les utiliser comme vecteur pour représenter les phrases 1 et 2. Ainsi, c'est cette méthode que l'on appelle *Bag-of-Words* : c'est une représentation de documents par des vecteurs dont la dimension est égale à la taille du vocabulaire, et qui est construit en comptant le nombre d'occurence de chaque mot. Ainsi, chaque token est ici associé à une dimension.\n",
        "\n",
        "\n",
        "##### Question 5. Implémentez le Bag-of-Words (2 Pts)\n",
        "\n",
        "Pour cette question, vous ne pouvez pas utiliser de librairie Python externe comme scikit-learn, hormis si vous avez des problèmes de mémoire, vous pouvez utiliser la classe sparse.csr_matrix de scipy (https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7e4f7BK1a_7m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.sparse import csr_matrix\n",
        "import operator\n",
        "\n",
        "class CountBoW(object):\n",
        "\n",
        "    def __init__(self, pipeline, bigram=False, trigram=False):\n",
        "        \"\"\"\n",
        "        pipelineObj: instance of PreprocesingPipeline\n",
        "        bigram: enable or disable bigram\n",
        "        trigram: enable or disable trigram\n",
        "        words: list of words in the vocabulary\n",
        "        \"\"\"\n",
        "        self.pipeline = pipeline\n",
        "        self.bigram = bigram\n",
        "        self.trigram = trigram\n",
        "        self.words = None\n",
        "\n",
        "    def computeBoW(self, tokens):\n",
        "        \"\"\"\n",
        "        Calcule du BoW, à partir d'un dictionnaire de mots et d'une liste de tweets.\n",
        "        On suppose que l'on a déjà collecté le dictionnaire sur l'ensemble d'entraînement.\n",
        "        \n",
        "        Entrée: tokens, une liste de vecteurs contenant les tweets (une liste de liste)\n",
        "        \n",
        "        Return: une csr_matrix\n",
        "        \"\"\"\n",
        "        \n",
        "        if self.words is None:\n",
        "            raise Exception(\n",
        "                \"fit_transform() should be called first (no dictionnary available)\"\n",
        "            )\n",
        "        \n",
        "        # TODO\n",
        "        \n",
        "        bagOfWords = csr_matrix((len(tokens), len(self.words)), dtype=np.int8).toarray()\n",
        "        \n",
        "        for tweet in range (0, len(tokens)):\n",
        "          for word in range(0, len(tokens[tweet])):\n",
        "            if(self.bigram):\n",
        "              if(word + 1 < len(tokens[tweet])):\n",
        "                bigramCombo = tokens[tweet][word] + \" \" + tokens[tweet][word + 1]\n",
        "                if(bigramCombo in self.words):\n",
        "                  bagOfWords[tweet, self.words.index(bigramCombo)] += 1\n",
        "            if(self.trigram):\n",
        "              if(word + 2 < len(tokens[tweet])):\n",
        "                trigramCombo = tokens[tweet][word] + \" \" + tokens[tweet][word + 1] + \" \" + tokens[tweet][word + 2]\n",
        "                if(trigramCombo in self.words):\n",
        "                  bagOfWords[tweet, self.words.index(trigramCombo)] += 1\n",
        "            if(tokens[tweet][word] not in self.words):\n",
        "              bagOfWords[tweet, self.words.index(\"<UNK>\")] += 1\n",
        "            else:\n",
        "              bagOfWords[tweet, self.words.index(tokens[tweet][word])] += 1\n",
        "              \n",
        "        \n",
        "        return bagOfWords\n",
        "\n",
        "    def fit_transform(self, X):\n",
        "        \"\"\"\n",
        "        Cette méthode preprocess les données en utilisant la pipeline, ajoute les bigram et trigram \n",
        "        si besoin, et transforme les textes en vecteurs d'entiers.\n",
        "        \n",
        "        Entrée: X, une liste de vecteurs contenant les tweets\n",
        "        \n",
        "        Return: une csr_matrix\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "        X_transformed = []\n",
        "        for tweet in X:\n",
        "          X_transformed.append(pipeline.preprocess(tweet))\n",
        "          \n",
        "        if(self.bigram):\n",
        "          bigrams = []\n",
        "          for tweet in X_transformed:\n",
        "            bigrams.append(bigram(tweet))\n",
        "        \n",
        "        if(self.trigram):\n",
        "          trigrams = []\n",
        "          for tweet in X_transformed:\n",
        "            trigrams.append(trigram(tweet))\n",
        "        \n",
        "        \n",
        "        \n",
        "        #dictionnaire\n",
        "        dictionary = {}\n",
        "        for tweet in range (0, len(X_transformed)):\n",
        "          for word in range(0, len(X_transformed[tweet])):\n",
        "            if(X_transformed[tweet][word] not in dictionary):\n",
        "              dictionary[X_transformed[tweet][word]] = 1\n",
        "            else:\n",
        "              dictionary[X_transformed[tweet][word]] = dictionary[X_transformed[tweet][word]] + 1\n",
        "              \n",
        "              \n",
        "        if(self.bigram):\n",
        "          for tweet in range (0, len(bigrams)):\n",
        "            for word in range(0, len(bigrams[tweet])):\n",
        "              if(bigrams[tweet][word] not in dictionary):\n",
        "                dictionary[bigrams[tweet][word]] = 1\n",
        "              else:\n",
        "                dictionary[bigrams[tweet][word]] = dictionary[bigrams[tweet][word]] + 1\n",
        "                \n",
        "        if(self.trigram):\n",
        "          for tweet in range (0, len(trigrams)):\n",
        "            for word in range(0, len(trigrams[tweet])):\n",
        "              if(trigrams[tweet][word] not in dictionary):\n",
        "                dictionary[trigrams[tweet][word]] = 1\n",
        "              else:\n",
        "                dictionary[trigrams[tweet][word]] = dictionary[trigrams[tweet][word]] + 1\n",
        "        \n",
        "        sortedArray = sorted(dictionary.items(), key=operator.itemgetter(1), reverse=True)[:5000]\n",
        "        sorted_keys = [ item[0] for item in sortedArray ]\n",
        "        self.words = sorted_keys\n",
        "        \n",
        "        #enlever les mots qui ne sont pas dans le dictionnaire\n",
        "        '''\n",
        "        for tweet in range (0, len(X_transformed)):\n",
        "          for word in range(0, len(X_transformed[tweet])):\n",
        "            if(X_transformed[tweet][word] not in self.words):\n",
        "              X_transformed[tweet][word] = \"<UNK>\"\n",
        "        '''\n",
        "        self.words.append(\"<UNK>\")\n",
        "        \n",
        "        bOfW = self.computeBoW(X_transformed)\n",
        "        \n",
        "        return bOfW\n",
        "        \n",
        "        \n",
        "    \n",
        "    def transform(self, X):\n",
        "        \"\"\"\n",
        "        Cette méthode preprocess les données en utilisant la pipeline, ajoute les bigram et trigram \n",
        "        si besoin, et transforme les textes en vecteurs d'entiers.\n",
        "        Différence avec fit_transform : on suppose qu'on dispose déjà du dictionnaire ici\n",
        "\n",
        "        Entrée: X, une liste de vecteurs contenant les tweets\n",
        "        \n",
        "        Return: une csr_matrix\n",
        "        \"\"\"\n",
        "        \n",
        "        if self.words is None:\n",
        "            raise Exception(\n",
        "                \"fit_transform() should be called first (no dictionnary available)\"\n",
        "            )\n",
        "\n",
        "        # TODO\n",
        "        \n",
        "        X_transformed = []\n",
        "        for tweet in X:\n",
        "          X_transformed.append(pipeline.preprocess(tweet))\n",
        "        \n",
        "        bOfW = self.computeBoW(X_transformed)\n",
        "        \n",
        "        return bOfW\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HA5SVCiaa_7o",
        "colab_type": "text"
      },
      "source": [
        "## 6. TF-IDF\n",
        "\n",
        "L'utilisation de la fréquence d'apparition brute des mots, comme c'est le cas avec le bag-of-words, peut être problématique. En effet, peu de tokens auront une fréquence très élevée dans un document, et de ce fait, le poids de ces mots sera beaucoup plus grand que les autres, ce qui aura tendance à biaiser l'ensemble des poids. De plus, les mots qui apparaissent dans la plupart des documents n'aident pas à les discriminer. Par exemple, le mot \"*de*\" apparaît dans beaucoup de tweets de la base de données, et pour autant, avoir ce mot en commun ne permet pas de conclure que des tweets sont similaires. Au contraire, le mot \"*génial*\" est plus rare, mais les documents qui contiennent ce mot sont plus susceptibles d'être positif. TF-IDF est donc une méthode qui permet de pallier à ce problème.\n",
        "\n",
        "TF-IDF pondère le vecteur en utilisant une fréquence de document inverse (IDF) et une fréquence de termes (TF).\n",
        "\n",
        "TF est l'information locale sur l'importance qu'a un mot dans un document donné, tandis que IDF mesure la capacité de discrimination des mots dans un jeu de données. \n",
        "\n",
        "L'IDF d'un mot se calcule de la façon suivante:\n",
        "\n",
        "\\begin{equation}\n",
        "\t\\text{idf}_i = \\log\\left( \\frac{N}{\\text{df}_i} \\right),\n",
        "\\end{equation}\n",
        "\n",
        "avec $N$ le nombre de documents dans la base de donnée, et $\\text{df}_i$ le nombre de documents qui contiennent le mot $i$.\n",
        "\n",
        "Le nouveau poids $w_{ij}$ d'un mot $i$ dans un document $j$ peut ensuite être calculé de la façon suivante:\n",
        "\n",
        "\\begin{equation}\n",
        "\tw_{ij} = \\text{tf}_{ij} \\times \\text{idf}_i,\n",
        "\\end{equation}\n",
        "\n",
        "avec $\\text{tf}_{ij}$ la fréquence du mot $i$ dans le document $j$.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##### Question 6. Implémentez le bag-of-words avec la pondération de TF-IDF (3 Pts)\n",
        "\n",
        "Pour cette question, vous ne pouvez pas utiliser de librairie Python externe comme scikit-learn, hormis si vous avez des problèmes de mémoire, vous pouvez utiliser la classe sparse.csr_matrix de scipy (https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXoei9kFa_7p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.sparse import csr_matrix\n",
        "import math\n",
        "\n",
        "\n",
        "class TFIDFBoW(object):\n",
        "\n",
        "    def __init__(self, pipeline, bigram=False, trigram=False):\n",
        "        \"\"\"\n",
        "        pipelineObj: instance of PreprocesingPipeline\n",
        "        bigram: enable or disable bigram\n",
        "        trigram: enable or disable trigram\n",
        "        words: list of words in the vocabulary\n",
        "        idf: list of idfs for each document\n",
        "        \"\"\"\n",
        "        self.pipeline = pipeline\n",
        "        self.bigram = bigram\n",
        "        self.trigram = trigram\n",
        "        self.words = None\n",
        "        self.idf = None\n",
        "    \n",
        "    def computeTFIDF(self, X):\n",
        "        \"\"\"\n",
        "        Calcule du TF-IDF, à partir d'un dictionnaire de mots et d'une \n",
        "        liste de tweets.\n",
        "        On suppose que l'on a déjà collecté le dictionnaire ainsi que \n",
        "        calculé le vecteur contenant l'idf pour chaque document.\n",
        "        \n",
        "        Entrée : X, une liste de vecteurs contenant les tweets\n",
        "        \n",
        "        Return: une csr_matrix\n",
        "        \"\"\"\n",
        "        \n",
        "        if self.words is None:\n",
        "            raise Exception(\n",
        "                \"fit_transform() should be called first (no dictionnary available)\"\n",
        "            )\n",
        "        \n",
        "        # TODO\n",
        "\n",
        "\n",
        "    def fit_transform(self, X):\n",
        "        \"\"\"\n",
        "        Cette méthode preprocess les données en utilisant la pipeline, ajoute les bigram et trigram \n",
        "        si besoin, et transforme les textes en vecteurs de flottants avec la pondération TF-IDF.\n",
        "        \n",
        "        Entrée : X, une liste de vecteurs contenant les tweets\n",
        "        \n",
        "        Return: une csr_matrix\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"\n",
        "        Cette méthode preprocess les données en utilisant la pipeline, ajoute les bigram et trigram \n",
        "        si besoin, et transforme les textes en vecteurs de flottants avec la pondération TF-IDF.\n",
        "        Différence avec fit_transform : on suppose qu'on dispose déjà du dictionnaire et du calcul des idf ici.\n",
        "            \n",
        "        Entrée : X, une liste de vecteurs contenant les tweets\n",
        "        \n",
        "        Return: une csr_matrix\n",
        "        \"\"\"\n",
        "\n",
        "        if self.words is None:\n",
        "            raise Exception(\n",
        "                \"fit_transform() should be called first (no dictionnary available)\"\n",
        "            )\n",
        "\n",
        "        # TODO\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vCEQSILa_7s",
        "colab_type": "text"
      },
      "source": [
        "## 7. Classification utilisant BoW\n",
        "\n",
        "Pour la classification, nous allons effectuer une régression logisitique (vu en cours ou que vous allez voir bientôt). \n",
        "Pour en savoir plus : https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
        "\n",
        "La méthode `train_evaluate` entraîne et évalue le modèle de régression logistique.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzaUEDqDa_7t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "def train_evaluate(training_X, training_Y, validation_X, validation_Y, bowObj):\n",
        "    \"\"\"\n",
        "    training_X: tweets from the training dataset\n",
        "    training_Y: tweet labels from the training dataset\n",
        "    validation_X: tweets from the validation dataset\n",
        "    validation_Y: tweet labels from the validation dataset\n",
        "    bowObj: Bag-of-word object\n",
        "    \n",
        "    :return: the classifier and its accuracy in the training and validation dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    classifier = LogisticRegression(n_jobs=-1)\n",
        "\n",
        "    training_rep = bowObj.fit_transform(training_X)\n",
        "\n",
        "    print(training_rep)\n",
        "    classifier.fit(training_rep, training_Y)\n",
        "\n",
        "    trainAcc = accuracy_score(training_Y, classifier.predict(training_rep))\n",
        "    validationAcc = accuracy_score(\n",
        "        validation_Y, classifier.predict(bowObj.transform(validation_X)))\n",
        "\n",
        "    return classifier, trainAcc, validationAcc\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTuZo6bPa_7v",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "##### Question 7. Entraînez et calculez la précision de la régression logistique sur les ensembles d'entraînement et de validation. (4 points)\n",
        "\n",
        "Essayez les configurations suivantes :\n",
        "\n",
        "    1. CountBoW + SpaceTokenizer(without tokenizer) + unigram \n",
        "    2. CountBoW + NLTKTokenizer + unigram\n",
        "    3. TFIDFBoW + NLTKTokenizer + Stemming + unigram\n",
        "    4. TFIDFBoW + NLTKTokenizer + Twitter preprocessing + Stemming  + unigram\n",
        "    5. TFIDFBoW + NLTKTokenizer + Twitter preprocessing + Stemming  + unigram + bigram\n",
        "    6. TFIDFBoW + NLTKTokenizer + Twitter preprocessing + Stemming  + unigram + bigram + trigram\n",
        "\n",
        "Outre la précision, reportez la taille du dictionnaire pour chacune des configurations. Enfin, décrivez vos résultats obtenus et répondez aux questions suivantes:\n",
        "- Quelles étapes de preprocessing ont effectivement aidé le modèle ? Pourquoi ?\n",
        "- La pondération avec TF-IDF a-t-elle aidé à obtenir une meilleure performance que le simple BoW ?\n",
        "- Les bigrams et trigrams ont-ils amélioré la performance ? Expliquez pourquoi.\n",
        "\n",
        "Indiquez quelle est la configuration que vous choisissez."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAs4KbmLa_7v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "0f06f6db-901b-4bda-9b8e-130fae218d92"
      },
      "source": [
        "bagOfWords = CountBoW(pipeline, bigram=True, trigram=True)\n",
        "train_evaluate(train_X, train_Y, valid_X, valid_Y, bagOfWords)\n",
        "\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2 4 0 ... 0 0 1]\n",
            " [0 0 0 ... 0 0 2]\n",
            " [0 0 1 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 3]\n",
            " [1 0 0 ... 0 0 4]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:1544: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                    multi_class='warn', n_jobs=-1, penalty='l2',\n",
              "                    random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
              "                    warm_start=False), 0.9494315954527637, 0.8044642857142857)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdSh6KL6a_7x",
        "colab_type": "text"
      },
      "source": [
        "# II/ Prototype (7 points)\n",
        "\n",
        "Maintenant que nous avons un modèle de classification entraîné pour l'analyse de sentiments, nous pouvons l'appliquer à notre ensemble de tests et analyser le résultat.\n",
        "\n",
        "## 1. Analyse de Sentiments\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UtAWfRca_7y",
        "colab_type": "text"
      },
      "source": [
        "##### Question 9. Implémentez la fonction `detect_airline` qui détecte la compagnie aérienne d'un tweet. (1,5 points)\n",
        "\n",
        "Expliquez votre approche, et les inconvénients possibles.\n",
        "\n",
        "**Attention :** `detect_airline` doit être en mesure de gérer le cas où aucune compagnie n'est mentionnée (auquel cas `None` est retounée), mais aussi le cas où plusieurs compagnies sont mentionnées dans un tweet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFfXlzEza_7y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def detect_airline(tweet):\n",
        "    \"\"\"\n",
        "    Detect and return the airline companies mentioned in the tweet\n",
        "    \n",
        "    tweet: represents the tweet message. You should define the data type\n",
        "    \n",
        "    Return: list of detected airline companies\n",
        "    \"\"\"\n",
        "    \n",
        "    # TODO\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsOdjy6Ka_70",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "##### Question 10. Implémentez la fonction `extract_sentiment` qui, à partir de tweets et d'un classificateur, extrait leurs sentiments. (0.5 points)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoA0LOLOa_71",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_sentiment(classifier, tweets):\n",
        "    \"\"\"\n",
        "    Extract the tweet sentiment\n",
        "    \n",
        "    classifier: classifier object\n",
        "    tweet: represents the tweet message. You should define the data type\n",
        "    \n",
        "    Return: list of detected airline companies\n",
        "    \"\"\"\n",
        "    # TODO\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gAQqCvOa_74",
        "colab_type": "text"
      },
      "source": [
        "##### Question 11. En utilisant `extract_tweet_content`, `detect_airline` et `extract_sentiment`, générez un diagramme en bar contenant le nombre de tweets positives, négatifs et neutres pour chacune des compagnies. (2 points)\n",
        "\n",
        "Décrivez brièvement le diagramme et analysez les résultats (par exemple, quelle est la compagnie avec le plus de tweets négatifs?). Expliquez comment un tel diagramme peut aider des compagnies aériennes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yz7kXTFa_75",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptOv5aksa_78",
        "colab_type": "text"
      },
      "source": [
        "## 2. Analyse de termes\n",
        "\n",
        "Le POS-tagging (pour *part-of-speech tagging*, en français étiquetage grammatical) consiste à l'extraction de l'information grammaticale d'un token dans une phrase. Par exemple, la table ci-dessous donne un exemple du *POS-tagging* de la phrase *\"The cat is white!\"*\n",
        "\n",
        "\n",
        "|   The   | cat  |  is  | white     |    !       |\n",
        "|---------|------|------|-----------|------------|\n",
        "| article | noun | verb | adjective | punctation |\n",
        "\n",
        "\n",
        "Pour autant, le *POS-tagging* peut être plus complexe que les règles simples apprises à l'école. Il faut souvent des informations plus détaillées sur le rôle d'un terme dans une phrase. Pour notre problème, nous n'avons pas besoin d'utiliser un modèle linguistique plus complexe, nous allons utiliser ce qu'on appelle des *POS-tags* universelles.\n",
        "\n",
        "En *POS-tagging*, chaque token est représenté par un tag. La liste des POS-tags utilisés sont disponibles ici :\n",
        "https://universaldependencies.org/u/pos/ ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-zLXhCCa_78",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# NLTK POS-tagger\n",
        "\n",
        "import nltk\n",
        "\n",
        "\n",
        "#before using pos_tag function, you have to tokenize the sentence.\n",
        "s = ['The', 'cat', 'is',  'white', '!']\n",
        "nltk.pos_tag(s,tagset='universal')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjQCCnvHa_7-",
        "colab_type": "text"
      },
      "source": [
        "##### Question 12. Implémentez un code qui collecte les 10 termes les plus fréquents pour chaque compagnie aérienne. (2 Pts)\n",
        "\n",
        "Ici, vous n'allez considérer que les termes apparaissant dans les tweets positifs et négatifs. \n",
        "\n",
        "De plus, nous allons utiliser la définition suivante de \"terme\":\n",
        "\n",
        "1. Un mot qui est soit un adjectif, soit un nom\n",
        "2. Un N-gram composé d'adjectifs suivit par un nom (par exemple, \"nice place\"), ou un nom suivi par un autre nom (par exemple, \"sports club\").\n",
        "\n",
        "Ensuite, **générez une table** contenant les 10 termes les plus fréquents, avec leurs fréquences (en pourcentage) pour chaque compagnie.\n",
        "\n",
        "*N'oubliez pas de supprimer le nom de la compagnie parmi les termes !*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuODfwUva_7_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvLJeZj1a_8B",
        "colab_type": "text"
      },
      "source": [
        "##### Question 13. Que conclure de la table généré à la question 12 pour les compagnies ? (1 Pt)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtOFYZNea_8C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GenYQyNYa_8D",
        "colab_type": "text"
      },
      "source": [
        "# III/ Bonus (2 points)\n",
        "\n",
        "Les noms de personnes, les noms de sociétés et les emplacements sont appelés \"entités nommées\". La reconnaissance d'entité nommée (NER, pour *Named-entity recognition*) consiste à extraire les entités nommées en les classant à l'aide de catégories prédéfinies. Dans cette section bonus, vous utiliserez un outil de NER pour extraire automatiquement des entités nommées des tweets. Cette approche est suffisamment générique pour récupérer des informations sur d’autres sociétés ou même des noms de produits et de personnes.\n",
        "\n",
        "\n",
        "**Pour le bonus, vous êtes libres d'utiliser n'importe quel NER implémenté en Python.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUHZnFUVa_8E",
        "colab_type": "text"
      },
      "source": [
        "##### Question Bonus 1.  Implémentez un code qui génère une table contenant le top 10 des NER de la base de données. (1 point)\n",
        "\n",
        "Cette table doit contenir les fréquences des entités nommées. Ensuite, générez un diagramme en bar qui montre le nombre de tweets positifs, négatifs ou neutres pour chacunes des 10 NER. Décrivez le résultat obtenu.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhU5iA4la_8E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBiPE2f8a_8G",
        "colab_type": "text"
      },
      "source": [
        "##### Question Bonus 2. Générez une table similaire à la question 12 pour le top 10 des NER pour chaque compagnie. (1 point)\n",
        "\n",
        "Que peut-on conclure de ces résultats?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOtNpIKMa_8H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}