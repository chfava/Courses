{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "INF8953CE_Kaggle_(6)_(1)_(1)_(1)_(2) (2) (1).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "environment": {
      "name": "tf2-2-3-gpu.2-3.m59",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m59"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chfava/INF8953CE_Kaggle/blob/main/INF8953CE_Kaggle_(6)_(1)_(1)_(1)_(2)_(2)_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3vxscHCLtx3"
      },
      "source": [
        "# Connect Google Drive for Kaggle config file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8ciXc1b43LS"
      },
      "source": [
        "\n",
        "\n",
        "1.  From your Kaggle account, create New API Token and download file\n",
        "2. Create directory /Kaggle at the highest level in your Google Drive (My Drive/Kaggle)\n",
        "3.  Place the kaggle.json file in My Drive/Kaggle/\n",
        "4. Link the colab to your google drive in the following cell \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "om9Tuj4dGBu1",
        "outputId": "db8e8c2e-26be-4581-e8dd-378d7b4295e5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3ZhJwR4JbDp"
      },
      "source": [
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/My Drive/Kaggle\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_n_2-jIgsze"
      },
      "source": [
        "PATH = 'gdrive/My Drive/Kaggle/'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RheZiznNgsze"
      },
      "source": [
        "#PATH = ''"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4GuDXcHHII8"
      },
      "source": [
        "# Download Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQ2KshkAHMWd"
      },
      "source": [
        "%%capture\n",
        "!kaggle competitions download -c f2020-INF8953CE"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0UrrST7KZew"
      },
      "source": [
        "%%capture\n",
        "!unzip -o  \\*.zip"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wR9ZorBtKOOl"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "from keras.layers.merge import add, concatenate, Multiply\n",
        "from keras import regularizers\n",
        "import cv2\n",
        "from scipy.ndimage import gaussian_filter\n",
        "from scipy.ndimage import map_coordinates\n",
        "import json\n",
        "from multiprocessing import Process\n",
        "from multiprocessing import Queue"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDUD65YyfSyV"
      },
      "source": [
        "class Dataset:\n",
        "  def __init__(self, name, preprocess = False, augment = False, deform_augment = False, alpha = [5], sigma = [2], rotate_augment = False, angles = [60], flipAND_ = False):\n",
        "    self.name = name\n",
        "    self.isPreprocessed = preprocess\n",
        "    self.alpha = alpha\n",
        "    self.sigma = sigma\n",
        "    self.angles = angles\n",
        "    self.IMAGE_SIZE_CROP = 32\n",
        "    self.IMAGE_SIZE = 100\n",
        "\n",
        "    self.oneHotTransformer = OneHotEncoder(handle_unknown='ignore', sparse = False)\n",
        "    if preprocess == False:\n",
        "      self.X_train = self.convertData(np.load('train_images.npy',allow_pickle=True, encoding = 'bytes'))\n",
        "      self.X_submission = self.convertData(np.load('test_images.npy', allow_pickle=True, encoding = 'bytes'))\n",
        "      self.Y_train = self.oneHotTransformer.fit_transform(pd.read_csv(\"train_labels.csv\").Category.values.reshape(-1, 1))\n",
        "\n",
        "    elif preprocess == True:\n",
        "      self.X_train = self.convertData(self.preprocess(np.load('train_images.npy',allow_pickle=True, encoding = 'bytes'), r = int(self.IMAGE_SIZE_CROP/2)))\n",
        "      self.X_submission = self.convertData(self.preprocess(np.load('test_images.npy', allow_pickle=True, encoding = 'bytes'), r = int(self.IMAGE_SIZE_CROP/2)))\n",
        "      self.Y_train = self.oneHotTransformer.fit_transform(pd.read_csv(\"train_labels.csv\").Category.values.reshape(-1, 1))\n",
        "\n",
        "    self.splitData()\n",
        "\n",
        "    self.prepareForFinalTrain()\n",
        "    \n",
        "    X_train_original = self.X_train\n",
        "    Y_train_original = self.Y_train\n",
        "\n",
        "    if augment == True:\n",
        "      self.X_train = np.asarray(list(self.flip(self.X_train)) + list(self.X_train))\n",
        "      self.Y_train = np.asarray(list(self.Y_train) + list(self.Y_train))\n",
        "\n",
        "    if flipAND_ == True:\n",
        "      X_train_original = self.X_train\n",
        "      Y_train_original = self.Y_train\n",
        "\n",
        "    if deform_augment == True:\n",
        "      print(\"Augmentation of the dataset by deformation\")\n",
        "      Y_list = []\n",
        "      q = Queue()\n",
        "      p_ = []\n",
        "      for i in range(0,len(self.alpha)):\n",
        "        p = Process(target=self.deformation, args=(q, X_train_original, self.alpha[i], self.sigma[i], None))\n",
        "        p_.append(p)\n",
        "        p.start()\n",
        "        print('*', end='')\n",
        "        Y = np.asarray(list(Y_train_original))\n",
        "        Y_list.append(Y)\n",
        "\n",
        "      for i in range(0,len(Y_list)):\n",
        "        self.X_train = np.asarray(list(self.X_train) + list(q.get()))\n",
        "        self.Y_train = np.asarray( list(self.Y_train) + list(Y_list[i]))\n",
        "      \n",
        "      print('\\n preparing for join')\n",
        "      for p in p_:\n",
        "        p.join()\n",
        "        print('*', end='')\n",
        "\n",
        "    print('\\n')\n",
        "\n",
        "    if rotate_augment == True:\n",
        "      print(\"Augmentation of the dataset by rotation\")\n",
        "      \n",
        "      Y_list = []\n",
        "      q = Queue()\n",
        "      p_ = []\n",
        "      for angle in self.angles:\n",
        "        p = Process(target=self.applyRotationToDataset, args=(X_train_original, angle, q,))\n",
        "        p_.append(p)\n",
        "        p.start()\n",
        "        print('*', end='')\n",
        "        Y = np.asarray(list(Y_train_original))\n",
        "        Y_list.append(Y)\n",
        "      print('\\n preparing for join')\n",
        "      \n",
        "      for i in range(0,len(Y_list)):\n",
        "        self.X_train = np.asarray(list(self.X_train) + list(q.get()))\n",
        "        self.Y_train = np.asarray( list(self.Y_train) + list(Y_list[i]))\n",
        "\n",
        "      for p in p_:\n",
        "        p.join()\n",
        "        print('*', end='')\n",
        "        \n",
        "  def convertData(self, data):\n",
        "\n",
        "    images = []\n",
        "    for dataLine in data:\n",
        "      if self.isPreprocessed == False:\n",
        "        images.append(np.reshape(dataLine[1], (self.IMAGE_SIZE, self.IMAGE_SIZE, 1)))\n",
        "      else:\n",
        "        images.append(np.reshape(dataLine, (self.IMAGE_SIZE_CROP, self.IMAGE_SIZE_CROP, 1)))\n",
        "    return np.asarray(images)\n",
        "\n",
        "  def visualizeData(self, indexBegin, indexEnd, dataset = 'X_train'):\n",
        "    if dataset == 'X_train':\n",
        "      data = self.X_train\n",
        "    elif dataset == 'X_valid':\n",
        "      data = self.X_valid\n",
        "    elif dataset == 'X_test':\n",
        "      data = self.X_test\n",
        "    elif dataset == 'X_submission':\n",
        "      data = self.X_submission\n",
        "    \n",
        "    for index in range(indexBegin, indexEnd):\n",
        "      plt.imshow(np.reshape(data[index], (self.IMAGE_SIZE_CROP,self.IMAGE_SIZE_CROP)), cmap='Greys')\n",
        "      plt.show()\n",
        "\n",
        "  def splitData(self):\n",
        "    self.X_train, self.X_test, self.Y_train, self.Y_test = train_test_split(self.X_train, self.Y_train, test_size=0.10, random_state=42)\n",
        "    self.X_train, self.X_valid, self.Y_train, self.Y_valid = train_test_split(self.X_train, self.Y_train, test_size=0.20, random_state=42)\n",
        "  \n",
        "  def pre2(self, img, blur= True, dilate= False, min_area=70):\n",
        "    blackAndWhite = img\n",
        "    # Blur filter\n",
        "    if blur:\n",
        "      blackAndWhite = cv2.medianBlur(img, 3)\n",
        "\n",
        "    # Find connectected components\n",
        "    nlabels, labels, stats, centroids = cv2.connectedComponentsWithStats(blackAndWhite, 8, cv2.CV_8U)\n",
        "    sizes = stats[1:, -1] #get CC_STAT_AREA component\n",
        "    img2 = np.zeros((labels.shape), np.uint8)\n",
        "\n",
        "    for i in range(0, nlabels - 1):\n",
        "        if sizes[i] >= min_area:   #filter small dotted regions\n",
        "            img2[labels == i + 1] = 255\n",
        "\n",
        "    dilation = img2\n",
        "    # Dilation of the mask\n",
        "    if dilate:\n",
        "      kernel = np.ones((5,5),np.uint8)\n",
        "      dilation = cv2.dilate(img2, kernel, iterations = 1) \n",
        "    \n",
        "    # Mask kept regions\n",
        "    res = cv2.bitwise_and(img, dilation)\n",
        "    return res\n",
        "\n",
        "  def crop_img(self, img, r=15):\n",
        "    nlabels, labels, stats, centroids = cv2.connectedComponentsWithStats(img, 8, cv2.CV_8U)\n",
        "    tmp = []\n",
        "    for c in centroids:\n",
        "      x= int(round(c[0]))\n",
        "      y= int(round(c[1]))\n",
        "\n",
        "      start_x = x-r\n",
        "      end_x = x+r\n",
        "      if start_x < 0:\n",
        "        end_x = end_x - start_x\n",
        "        start_x = 0\n",
        "      elif end_x > 99:\n",
        "        start_x = 99 - 2*r\n",
        "        end_x = 99\n",
        "      \n",
        "      start_y = y-r\n",
        "      end_y = y+r\n",
        "      if start_y < 0:\n",
        "        end_y = end_y - start_y\n",
        "        start_y = 0\n",
        "      elif end_y > 99:\n",
        "        start_y = 99 - 2*r\n",
        "        end_y = 99\n",
        "\n",
        "      tmp.append(img[start_y:end_y, start_x:end_x])\n",
        "\n",
        "    return tmp[np.argmax([im.sum() for im in tmp])]\n",
        "\n",
        "  # Apply preprocess\n",
        "  def preprocess(self, dataset, r=15):\n",
        "    dataset = dataset[:,1]\n",
        "    images = []\n",
        "    for img in dataset:\n",
        "      temp = img.astype(np.uint8).reshape(self.IMAGE_SIZE,self.IMAGE_SIZE)\n",
        "      \n",
        "      res = self.pre2(temp)\n",
        "      res = self.pre2(res, blur=False)\n",
        "      res = self.crop_img(res, r)\n",
        "        \n",
        "      images.append(res.reshape(4*r*r,))\n",
        "      images_np = np.array(images)\n",
        "\n",
        "    return images_np\n",
        "\n",
        "  def flip(self, dataset):\n",
        "    dataset_flip = []\n",
        "    for img in dataset:\n",
        "      img = np.reshape(img, (self.IMAGE_SIZE_CROP, self.IMAGE_SIZE_CROP))\n",
        "      d_flip = cv2.flip(img, 1)\n",
        "      dataset_flip.append(np.reshape(d_flip, (self.IMAGE_SIZE_CROP, self.IMAGE_SIZE_CROP, 1)))\n",
        "\n",
        "    return np.asarray(dataset_flip)\n",
        "\n",
        "  def deformation(self, q, dataset, alpha = 5, sigma = 2, random_state=None ):\n",
        "    deform_dataset = []\n",
        "    for img in dataset:\n",
        "      img = np.reshape(img, (self.IMAGE_SIZE_CROP, self.IMAGE_SIZE_CROP))\n",
        "      shape = img.shape\n",
        "\n",
        "      if random_state is None:\n",
        "          random_state = np.random.RandomState(None)\n",
        "\n",
        "      dx = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma, mode=\"constant\", cval=0) * alpha\n",
        "      dy = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma, mode=\"constant\", cval=0) * alpha\n",
        "\n",
        "      x, y = np.meshgrid(np.arange(shape[0]), np.arange(shape[1]), indexing='ij')\n",
        "      indices = np.reshape(x+dx, (-1, 1)), np.reshape(y+dy, (-1, 1))\n",
        "      img_deform = map_coordinates(img, indices, order=1).reshape(shape)\n",
        "      img_deform = np.reshape(img_deform, (self.IMAGE_SIZE_CROP, self.IMAGE_SIZE_CROP, 1))\n",
        "      deform_dataset.append(img_deform)\n",
        "    \n",
        "    q.put(np.asarray(deform_dataset))\n",
        "  \n",
        "  def applyRotationToDataset(self, dataset, angle, q):\n",
        "    rotated_dataset = []\n",
        "    for img in dataset:\n",
        "      img = np.reshape(img, (self.IMAGE_SIZE_CROP, self.IMAGE_SIZE_CROP))\n",
        "      img = self.rotate_image(img, angle)\n",
        "      rotated_dataset.append(np.reshape(img, (self.IMAGE_SIZE_CROP, self.IMAGE_SIZE_CROP, 1)))\n",
        "    q.put(np.asarray(rotated_dataset))\n",
        "\n",
        "  def rotate_image(self, image, angle):\n",
        "    center = tuple(np.array(image.shape[1::-1]) / 2)\n",
        "    matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
        "    result = cv2.warpAffine(image, matrix, image.shape[1::-1], flags=cv2.INTER_LINEAR)\n",
        "    return result\n",
        "\n",
        "  def prepareForFinalTrain(self):\n",
        "    self.X_ALL = np.asarray(list(self.X_train) + list(self.X_valid) + list(self.X_test))\n",
        "    self.Y_ALL = np.asarray(list(self.Y_train) + list(self.Y_valid) + list(self.Y_test))\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUxENwxV4sg4"
      },
      "source": [
        "#dataset = Dataset('Quick Draw')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBaPDcBFgeAZ"
      },
      "source": [
        "#dataset.visualizeData(0,7)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_J4uhj1kSUa"
      },
      "source": [
        "class TrainableModel:\n",
        "  def __init__(self, dataset, model, name = \"\", patience = 15):\n",
        "    self.name = name\n",
        "    self.dataset = dataset\n",
        "    self.model = model\n",
        "    self.es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=patience)\n",
        "    self.mc = ModelCheckpoint(PATH + 'best_model.h5', monitor='val_loss', mode='min', save_best_only=True)\n",
        "    self.optimizer = 'adam'\n",
        "    self.compileModel()\n",
        "\n",
        "  def evaluate(self):\n",
        "    #self.loadBestModel()\n",
        "    results = self.model.evaluate(self.dataset.X_test, self.dataset.Y_test, batch_size=128)\n",
        "    self.test_accuracy = results[1]\n",
        "    print(results)\n",
        "    with open(PATH + 'History_' + str(self.name) + '_' + str(self.dataset.name) + '_' + str(self.test_accuracy) + '.json', 'w') as f:\n",
        "      json.dump(self.history.history, f)\n",
        "    \n",
        "  def compileModel(self):\n",
        "    self.model.compile(optimizer= self.optimizer, loss=tf.keras.losses.categorical_crossentropy, metrics=['accuracy'])\n",
        "    \n",
        "  def softmaxVectorToOneHot(self, vector):\n",
        "    index = np.argmax(vector, axis=0)\n",
        "    oneHot = np.zeros(31)\n",
        "    oneHot[index] = 1\n",
        "    return oneHot\n",
        "\n",
        "  def submission(self):\n",
        "    #self.loadBestModel()\n",
        "    Y_predicted = self.model.predict(self.dataset.X_submission)\n",
        "    labels = []\n",
        "    for line in Y_predicted:\n",
        "      labels.append(self.dataset.oneHotTransformer.inverse_transform(self.softmaxVectorToOneHot(line).reshape(1, -1))[0][0])\n",
        "    df = pd.DataFrame()\n",
        "    df['Category'] = labels\n",
        "    df.insert(0, 'Id', range(0, len(df)))\n",
        "    df.to_csv(PATH + 'Submission-' + str(self.test_accuracy) + '.csv',index=False)\n",
        "\n",
        "  def fit(self, num_epochs = 300, batchSize = 32):\n",
        "    X = self.dataset.X_train\n",
        "    Y = self.dataset.Y_train\n",
        "    X_valid = self.dataset.X_valid\n",
        "    Y_valid = self.dataset.Y_valid\n",
        "    self.history = self.model.fit(X, Y, epochs=num_epochs, batch_size = batchSize, validation_data=(X_valid, Y_valid), callbacks=[self.es, self.mc])\n",
        "\n",
        "  def loadBestModel(self):\n",
        "    self.model = load_model(PATH + 'best_model.h5')\n",
        "\n",
        "  def fitOverAllData(self, num_epochs = 5):\n",
        "    X = self.dataset.X_ALL\n",
        "    Y = self.dataset.Y_ALL\n",
        "    self.history = self.model.fit(X, Y, epochs=num_epochs)\n",
        "\n",
        "  def graphFittingLoss(self):\n",
        "    plt.plot(self.history.history['loss'])\n",
        "    plt.plot(self.history.history['val_loss'])\n",
        "    plt.title('Model loss over the training steps')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'test'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "  def graphFittingAccuracy(self):\n",
        "    plt.plot(self.history.history['accuracy'])\n",
        "    plt.plot(self.history.history['val_accuracy'])\n",
        "    plt.title('Model accuracy over the training steps')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'test'], loc='upper left')\n",
        "    plt.show()"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzWXsbkLMZ6d"
      },
      "source": [
        "# CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuV-2c3mQW0d"
      },
      "source": [
        "RUN = False"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWRO791-CgrJ"
      },
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(64, (5, 5), activation='relu', input_shape=(100, 100, 1)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Dropout(0.25))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Conv2D(128, (5, 5), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Dropout(0.25))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Conv2D(256, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Dropout(0.25))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(256, activation='relu'))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(31, activation='softmax'))\n",
        "#model.summary()\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8eosLer7M4he"
      },
      "source": [
        "if RUN:\n",
        "  cnn = TrainableModel(dataset, model, name = \"CNN\")\n",
        "  cnn.fit()\n",
        "  cnn.evaluate()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8ahp0bGMf-Q"
      },
      "source": [
        "# Big CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCuJeuDNQhY2"
      },
      "source": [
        "RUN = False"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cR8DByYWZI2c"
      },
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), strides=(1,1), padding=\"same\", activation='relu', input_shape=(100, 100, 1)))\n",
        "model.add(layers.BatchNormalization(momentum=0.99))\n",
        "model.add(layers.Conv2D(64, (3, 3), strides=(2,2), padding=\"same\", activation='relu'))\n",
        "model.add(layers.BatchNormalization(momentum=0.99))\n",
        "model.add(layers.Conv2D(32, (1, 1), strides=(1,1), padding=\"same\", activation='relu'))\n",
        "model.add(layers.BatchNormalization(momentum=0.99))\n",
        "model.add(layers.Conv2D(64, (3, 3), strides=(1,1), padding=\"same\", activation='relu'))\n",
        "model.add(layers.BatchNormalization(momentum=0.99))\n",
        "model.add(layers.MaxPooling2D((2, 2), strides=(2,2), padding=\"same\"))\n",
        "model.add(layers.Dropout(0.4))\n",
        "\n",
        "model.add(layers.Conv2D(128, (3, 3), strides=(2,2), padding=\"same\", activation='relu'))\n",
        "model.add(layers.BatchNormalization(momentum=0.99))\n",
        "model.add(layers.Conv2D(64, (1, 1), strides=(1,1),padding=\"same\", activation='relu'))\n",
        "model.add(layers.BatchNormalization(momentum=0.99))\n",
        "model.add(layers.Conv2D(128, (3, 3), strides=(1,1), padding=\"same\", activation='relu'))\n",
        "model.add(layers.BatchNormalization(momentum=0.99))\n",
        "model.add(layers.MaxPooling2D((2, 2), strides=(2,2), padding=\"same\"))\n",
        "model.add(layers.Dropout(0.4))\n",
        "\n",
        "model.add(layers.Conv2D(64, (1, 1), strides=(1,1),padding=\"same\", activation='relu'))\n",
        "model.add(layers.BatchNormalization(momentum=0.99))\n",
        "model.add(layers.Conv2D(128, (3, 3), strides=(1,1), padding=\"same\", activation='relu'))\n",
        "model.add(layers.BatchNormalization(momentum=0.99))\n",
        "model.add(layers.MaxPooling2D((2, 2), strides=(2,2), padding=\"same\"))\n",
        "model.add(layers.Dropout(0.4))\n",
        "\n",
        "model.add(layers.Conv2D(256, (3, 3), strides=(2,2),padding=\"same\", activation='relu'))\n",
        "model.add(layers.BatchNormalization(momentum=0.99))\n",
        "model.add(layers.Conv2D(128, (1, 1), strides=(1,1),padding=\"same\", activation='relu'))\n",
        "model.add(layers.BatchNormalization(momentum=0.99))\n",
        "model.add(layers.Conv2D(256, (3, 3), strides=(1,1),padding=\"same\", activation='relu'))\n",
        "model.add(layers.BatchNormalization(momentum=0.99))\n",
        "model.add(layers.MaxPooling2D((2, 2), strides=(2,2), padding=\"same\"))\n",
        "model.add(layers.Dropout(0.4))\n",
        "\n",
        "model.add(layers.Conv2D(128, (1, 1), strides=(1,1),padding=\"same\", activation='relu'))\n",
        "model.add(layers.BatchNormalization(momentum=0.99))\n",
        "model.add(layers.Conv2D(256, (3, 3), strides=(1,1),padding=\"same\", activation='relu'))\n",
        "model.add(layers.BatchNormalization(momentum=0.99))\n",
        "model.add(layers.MaxPooling2D((2, 2), strides=(2,2), padding=\"same\"))\n",
        "model.add(layers.Dropout(0.4))\n",
        "\n",
        "\n",
        "model.add(layers.Conv2D(512, (3, 3), strides=(2,2),padding=\"same\", activation='relu'))\n",
        "model.add(layers.BatchNormalization(momentum=0.99))\n",
        "model.add(layers.Conv2D(256, (1, 1), strides=(1,1),padding=\"same\", activation='relu'))\n",
        "model.add(layers.BatchNormalization(momentum=0.99))\n",
        "model.add(layers.Conv2D(512, (3, 3), strides=(1,1),padding=\"same\", activation='relu'))\n",
        "model.add(layers.BatchNormalization(momentum=0.99))\n",
        "model.add(layers.MaxPooling2D((2, 2), strides=(2,2), padding=\"same\"))\n",
        "model.add(layers.Dropout(0.4))\n",
        "\n",
        "model.add(layers.Conv2D(256, (1, 1), strides=(1,1),padding=\"same\", activation='relu'))\n",
        "model.add(layers.BatchNormalization(momentum=0.99))\n",
        "model.add(layers.Conv2D(512, (3, 3), strides=(1,1),padding=\"same\", activation='relu'))\n",
        "model.add(layers.BatchNormalization(momentum=0.99))\n",
        "model.add(layers.MaxPooling2D((2, 2), strides=(2,2), padding=\"same\"))\n",
        "model.add(layers.Dropout(0.4))\n",
        "\n",
        "model.add(layers.Conv2D(1024, (3, 3), strides=(2,2),padding=\"same\", activation='relu'))\n",
        "model.add(layers.BatchNormalization(momentum=0.99))\n",
        "model.add(layers.Conv2D(512, (1, 1), strides=(1,1),padding=\"same\", activation='relu'))\n",
        "model.add(layers.BatchNormalization(momentum=0.99))\n",
        "model.add(layers.Conv2D(1024, (3, 3), strides=(1,1),padding=\"same\", activation='relu'))\n",
        "model.add(layers.BatchNormalization(momentum=0.99))\n",
        "model.add(layers.MaxPooling2D((2, 2), strides=(2,2), padding=\"same\"))\n",
        "model.add(layers.Dropout(0.4))\n",
        "\n",
        "model.add(layers.Conv2D(512, (1, 1), strides=(1,1),padding=\"same\", activation='relu'))\n",
        "model.add(layers.BatchNormalization(momentum=0.99))\n",
        "model.add(layers.Conv2D(1024, (3, 3), strides=(1,1),padding=\"same\", activation='relu'))\n",
        "model.add(layers.BatchNormalization(momentum=0.99))\n",
        "model.add(layers.MaxPooling2D((2, 2), strides=(2,2), padding=\"same\"))\n",
        "model.add(layers.Dropout(0.4))\n",
        "\n",
        "model.add(layers.Conv2D(512, (1, 1), strides=(1,1),padding=\"same\", activation='relu'))\n",
        "model.add(layers.BatchNormalization(momentum=0.99))\n",
        "model.add(layers.Conv2D(1024, (3, 3), strides=(1,1),padding=\"same\", activation='relu'))\n",
        "model.add(layers.BatchNormalization(momentum=0.99))\n",
        "model.add(layers.Conv2D(512, (1, 1), strides=(1,1),padding=\"same\", activation='relu'))\n",
        "model.add(layers.BatchNormalization(momentum=0.99))\n",
        "model.add(layers.Conv2D(1024, (3, 3), strides=(1,1),padding=\"same\", activation='relu'))\n",
        "model.add(layers.BatchNormalization(momentum=0.99))\n",
        "model.add(layers.Conv2D(512, (1, 1), strides=(1,1),padding=\"same\", activation='relu'))\n",
        "model.add(layers.BatchNormalization(momentum=0.99))\n",
        "model.add(layers.MaxPooling2D((2, 2), strides=(2,2), padding=\"same\"))\n",
        "model.add(layers.Dropout(0.4))\n",
        "\n",
        "model.add(layers.Conv2D(1024, (3, 3), strides=(1,1),padding=\"same\", activation='relu'))\n",
        "model.add(layers.BatchNormalization(momentum=0.99))\n",
        "model.add(layers.Conv2D(256, (1, 1), strides=(1,1),padding=\"same\", activation='relu'))\n",
        "model.add(layers.BatchNormalization(momentum=0.99))\n",
        "model.add(layers.MaxPooling2D((2, 2), strides=(2,2), padding=\"same\"))\n",
        "model.add(layers.Dropout(0.4))\n",
        "\n",
        "model.add(layers.Conv2D(256, (1, 1), strides=(1,1),padding=\"same\", activation='relu'))\n",
        "model.add(layers.BatchNormalization(momentum=0.99))\n",
        "model.add(layers.Conv2D(512, (3, 3), strides=(1,1),padding=\"same\", activation='relu'))\n",
        "model.add(layers.BatchNormalization(momentum=0.99))\n",
        "model.add(layers.Conv2D(256, (1, 1), strides=(1,1),padding=\"same\", activation='relu'))\n",
        "model.add(layers.BatchNormalization(momentum=0.99))\n",
        "model.add(layers.Conv2D(512, (3, 3), strides=(1,1),padding=\"same\", activation='relu'))\n",
        "model.add(layers.BatchNormalization(momentum=0.99))\n",
        "model.add(layers.Conv2D(256, (1, 1), strides=(1,1),padding=\"same\", activation='relu'))\n",
        "model.add(layers.BatchNormalization(momentum=0.99))\n",
        "model.add(layers.MaxPooling2D((2, 2), strides=(2,2), padding=\"same\"))\n",
        "model.add(layers.Dropout(0.4))\n",
        "\n",
        "model.add(layers.Conv2D(512, (3, 3), strides=(1,1),padding=\"same\", activation='relu'))\n",
        "model.add(layers.BatchNormalization(momentum=0.99))\n",
        "model.add(layers.Conv2D(256, (1, 1), strides=(1,1),padding=\"same\", activation='relu'))\n",
        "model.add(layers.BatchNormalization(momentum=0.99))\n",
        "model.add(layers.MaxPooling2D((2, 2), strides=(2,2), padding=\"same\"))\n",
        "model.add(layers.Dropout(0.4))\n",
        "\n",
        "model.add(layers.Conv2D(128, (1, 1), strides=(1,1),padding=\"same\", activation='relu'))\n",
        "model.add(layers.BatchNormalization(momentum=0.99))\n",
        "model.add(layers.MaxPooling2D((2, 2), strides=(2,2), padding=\"same\"))\n",
        "model.add(layers.Dropout(0.4))\n",
        "\n",
        "model.add(layers.Conv2D(128, (1, 1), strides=(1,1),padding=\"same\", activation='relu'))\n",
        "model.add(layers.BatchNormalization(momentum=0.99))\n",
        "model.add(layers.Conv2D(256, (3, 3), strides=(1,1),padding=\"same\", activation='relu'))\n",
        "model.add(layers.BatchNormalization(momentum=0.99))\n",
        "model.add(layers.Conv2D(128, (1, 1), strides=(1,1),padding=\"same\", activation='relu'))\n",
        "model.add(layers.BatchNormalization(momentum=0.99))\n",
        "model.add(layers.Conv2D(256, (3, 3), strides=(1,1),padding=\"same\", activation='relu'))\n",
        "model.add(layers.BatchNormalization(momentum=0.99))\n",
        "model.add(layers.Conv2D(128, (1, 1), strides=(1,1),padding=\"same\", activation='relu'))\n",
        "model.add(layers.BatchNormalization(momentum=0.99))\n",
        "model.add(layers.Conv2D(256, (3, 3), strides=(1,1),padding=\"same\", activation='relu'))\n",
        "model.add(layers.BatchNormalization(momentum=0.99))\n",
        "model.add(layers.MaxPooling2D((2, 2), strides=(2,2), padding=\"same\"))\n",
        "model.add(layers.Dropout(0.4))\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(512, activation='relu'))\n",
        "model.add(layers.BatchNormalization(momentum=0.99))\n",
        "model.add(layers.Dropout(0.4))\n",
        "model.add(layers.Dense(512, activation='relu'))\n",
        "model.add(layers.BatchNormalization(momentum=0.99))\n",
        "model.add(layers.Dropout(0.4))\n",
        "model.add(layers.Dense(256, activation='relu'))\n",
        "model.add(layers.BatchNormalization(momentum=0.99))\n",
        "model.add(layers.Dropout(0.4))\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "model.add(layers.BatchNormalization(momentum=0.99))\n",
        "model.add(layers.Dropout(0.4))\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.BatchNormalization(momentum=0.99))\n",
        "model.add(layers.Dropout(0.4))\n",
        "model.add(layers.Dense(31, activation='softmax'))\n",
        "#model.summary()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E94-K8jWM0Wz"
      },
      "source": [
        "if RUN:\n",
        "  bigCNN = TrainableModel(dataset, model, name = \"Big CNN\")\n",
        "  bigCNN.fit()\n",
        "  bigCNN.evaluate()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUnW5R0k3IG8"
      },
      "source": [
        "# DenseNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aa3ZBjTwQl9k"
      },
      "source": [
        "RUN = False"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTQz1KX_3N0k"
      },
      "source": [
        "def relu(x): return layers.Activation('relu')(x)\n",
        "def dropout(x, p): return layers.Dropout(p)(x) if p else x\n",
        "def bn(x): return layers.BatchNormalization(axis=-1)(x)\n",
        "def relu_bn(x): return relu(bn(x))\n",
        "\n",
        "def conv(x, nf, sz, wd, p):\n",
        "    x = layers.Conv2D(nf, (sz, sz), kernel_initializer='uniform', padding='same', \n",
        "                          kernel_regularizer = tf.keras.regularizers.l2(wd))(x)\n",
        "    return dropout(x,p)\n",
        "\n",
        "def conv_block(x, nf, bottleneck=False, p=None, wd=0):\n",
        "    x = relu_bn(x)\n",
        "    if bottleneck: x = relu_bn(conv(x, nf * 4, 1, wd, p))\n",
        "    return conv(x, nf, 3, wd, p)\n",
        "\n",
        "def dense_block(x, nb_layers, growth_rate, bottleneck=False, p=None, wd=0):\n",
        "    if bottleneck: nb_layers //= 2\n",
        "    for i in range(nb_layers):\n",
        "        b = conv_block(x, growth_rate, bottleneck=bottleneck, p=p, wd=wd)\n",
        "        x = concatenate([x,b], -1)\n",
        "    return x\n",
        "\n",
        "def transition_block(x, compression=1.0, p=None, wd=0):\n",
        "    nf = int(x.get_shape().as_list()[-1] * compression)\n",
        "    x = relu_bn(x)\n",
        "    x = conv(x, nf, 1, wd, p)\n",
        "    return layers.AveragePooling2D((2, 2), strides=(2, 2))(x)\n",
        "\n",
        "def create_dense_net(nb_classes, img_input, depth=40, nb_block=3, \n",
        "     growth_rate=12, nb_filter=16, bottleneck=False, compression=1.0, p=None, wd=0, activation='softmax'):\n",
        "    \n",
        "    assert activation == 'softmax' or activation == 'sigmoid'\n",
        "    assert (depth - 4) % nb_block == 0\n",
        "    nb_layers_per_block = int((depth - 4) / nb_block)\n",
        "    nb_layers = [nb_layers_per_block] * nb_block\n",
        "\n",
        "    x = conv(img_input, nb_filter, 3, wd, 0)\n",
        "    for i,block in enumerate(nb_layers):\n",
        "        x = dense_block(x, block, growth_rate, bottleneck=bottleneck, p=p, wd=wd)\n",
        "        if i != len(nb_layers)-1:\n",
        "            x = transition_block(x, compression=compression, p=p, wd=wd)\n",
        "\n",
        "    x = relu_bn(x)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    return layers.Dense(nb_classes, activation=activation, kernel_regularizer=regularizers.l2(wd))(x)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g53AtbrX3N5I"
      },
      "source": [
        "input = layers.Input(shape=(100, 100, 1))\n",
        "output = create_dense_net(31, input, depth = 16)\n",
        "model = models.Model(input, output)\n",
        "#model.summary()"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwQRJwew3N2-"
      },
      "source": [
        "if RUN:\n",
        "  denseNet = TrainableModel(dataset, model, name = \"DenseNet\")\n",
        "  denseNet.optimizer = tf.keras.optimizers.SGD(0.1, 0.9, nesterov=True)\n",
        "  denseNet.compileModel()\n",
        "  denseNet.fit()\n",
        "  denseNet.evaluate()\n",
        "  denseNet.submission()"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCnhdGLelZiF"
      },
      "source": [
        "# Densenet on preprocessed and augmented data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBlNqyeel5FH"
      },
      "source": [
        "RUN = True"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpAUh8G4hvEK"
      },
      "source": [
        "angles = np.arange(-45, 45, 30)\n",
        "alphas = [20, 10, 5]\n",
        "sigmas = [4, 4, 2]"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGB6wbvws38w",
        "outputId": "ebc40fb0-fcf2-45d6-d4aa-220e8b6eba20"
      },
      "source": [
        "dataset_Pre_Aug = Dataset('Data_flip_angles-45To45_5deform', True, True, True, alpha = alphas, sigma = sigmas, rotate_augment = True, angles = angles, flipAND_ = True)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Augmentation of the dataset by deformation\n",
            "***\n",
            " preparing for join\n",
            "***\n",
            "\n",
            "Augmentation of the dataset by rotation\n",
            "***\n",
            " preparing for join\n",
            "***"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XUEZ-XPTu47"
      },
      "source": [
        "#dataset_Pre_Aug.visualizeData(345590,345599)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JG8jPW3QVxpe"
      },
      "source": [
        "#dataset_Pre_Aug.oneHotTransformer.inverse_transform( dataset_Pre_Aug.Y_train[403190].reshape(1, -1))"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZivJK_bEsaQ5",
        "outputId": "5e12914a-ecf6-4bb6-bccb-51b3f0924702"
      },
      "source": [
        "len(dataset_Pre_Aug.X_train)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100800"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTOOAHixZapX",
        "outputId": "5d047864-81f5-40bc-bae7-76c925a09cf0"
      },
      "source": [
        "len(dataset_Pre_Aug.X_test)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Sy_qjujZeV9",
        "outputId": "7d35999d-e3e2-4aa1-a47b-e8cb6326bd2e"
      },
      "source": [
        "len(dataset_Pre_Aug.X_valid)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1800"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMyMBCl7lnBS"
      },
      "source": [
        "input = layers.Input(shape=(32, 32, 1))\n",
        "output = create_dense_net(31, input, depth=124, nb_filter=12, growth_rate=36, compression=0.2, bottleneck=True, p=0.2, wd=1e-4)\n",
        "model = models.Model(input, output)\n",
        "#model.summary()"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 817
        },
        "id": "c8-FtwV3lyHh",
        "outputId": "723b71e2-99ee-4c45-efc6-ab791cb81968"
      },
      "source": [
        "if RUN:\n",
        "  denseNet = TrainableModel(dataset_Pre_Aug, model, name = \"DenseNet40layersBottleneckCompressionDropout0.2\", patience = 35)\n",
        "  denseNet.optimizer = tf.keras.optimizers.SGD(0.5, 0.9, nesterov=True)\n",
        "  \n",
        "  denseNet.compileModel()\n",
        "  denseNet.fit(num_epochs = 5, batchSize = 32)\n",
        "\n",
        "  denseNet.optimizer = tf.keras.optimizers.SGD(0.01, 0.9, nesterov=True)\n",
        "  denseNet.compileModel()\n",
        "  denseNet.fit(num_epochs = 40, batchSize = 32)\n",
        "\n",
        "  denseNet.evaluate()\n",
        "  denseNet.submission()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "3150/3150 [==============================] - 459s 146ms/step - loss: 2.6921 - accuracy: 0.4139 - val_loss: 2.7254 - val_accuracy: 0.4272\n",
            "Epoch 2/5\n",
            "3150/3150 [==============================] - 450s 143ms/step - loss: 1.9132 - accuracy: 0.6317 - val_loss: 2.1995 - val_accuracy: 0.5806\n",
            "Epoch 3/5\n",
            "3150/3150 [==============================] - 451s 143ms/step - loss: 1.8382 - accuracy: 0.6784 - val_loss: 2.0711 - val_accuracy: 0.6372\n",
            "Epoch 4/5\n",
            "3150/3150 [==============================] - 450s 143ms/step - loss: 1.8009 - accuracy: 0.6981 - val_loss: 1.9759 - val_accuracy: 0.6706\n",
            "Epoch 5/5\n",
            "3150/3150 [==============================] - 456s 145ms/step - loss: 1.7903 - accuracy: 0.7071 - val_loss: 1.9115 - val_accuracy: 0.6750\n",
            "Epoch 1/40\n",
            "3150/3150 [==============================] - 453s 144ms/step - loss: 1.3855 - accuracy: 0.8069 - val_loss: 1.3539 - val_accuracy: 0.8178\n",
            "Epoch 2/40\n",
            "3150/3150 [==============================] - 450s 143ms/step - loss: 1.2091 - accuracy: 0.8354 - val_loss: 1.2762 - val_accuracy: 0.8161\n",
            "Epoch 3/40\n",
            "3150/3150 [==============================] - 449s 142ms/step - loss: 1.0899 - accuracy: 0.8474 - val_loss: 1.2055 - val_accuracy: 0.8222\n",
            "Epoch 4/40\n",
            "3150/3150 [==============================] - 448s 142ms/step - loss: 0.9857 - accuracy: 0.8590 - val_loss: 1.1523 - val_accuracy: 0.8311\n",
            "Epoch 5/40\n",
            "3150/3150 [==============================] - 452s 143ms/step - loss: 0.8910 - accuracy: 0.8708 - val_loss: 1.1204 - val_accuracy: 0.8278\n",
            "Epoch 6/40\n",
            "3150/3150 [==============================] - 449s 143ms/step - loss: 0.8066 - accuracy: 0.8830 - val_loss: 1.1193 - val_accuracy: 0.8194\n",
            "Epoch 7/40\n",
            "3150/3150 [==============================] - 452s 144ms/step - loss: 0.7301 - accuracy: 0.8935 - val_loss: 1.0922 - val_accuracy: 0.8194\n",
            "Epoch 8/40\n",
            "2855/3150 [==========================>...] - ETA: 41s - loss: 0.6645 - accuracy: 0.9047"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-327ea19e7ac4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mdenseNet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnesterov\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mdenseNet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompileModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0mdenseNet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchSize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mdenseNet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-8fd818bd49d0>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, num_epochs, batchSize)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mX_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mY_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY_valid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatchSize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mloadBestModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOQ9GsS5P_nd"
      },
      "source": [
        "#denseNet.fitOverAllData(num_epochs = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDWm3_RhPFOc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "outputId": "bf66fedd-0940-4965-c2d2-7c0e81d8e3df"
      },
      "source": [
        "denseNet.evaluate()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "57/57 [==============================] - 1s 17ms/step - loss: 1.3253 - accuracy: 0.8172\n",
            "[1.3253268003463745, 0.8172222375869751]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-fca567302941>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdenseNet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-b59a0979bb1b>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'History_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_accuracy\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m       \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'path' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qksTiYdtHIpQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTcvqVdcwd5Y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}